{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install modal\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-13T17:50:51.830178Z","iopub.execute_input":"2024-10-13T17:50:51.830468Z","iopub.status.idle":"2024-10-13T17:51:24.153178Z","shell.execute_reply.started":"2024-10-13T17:50:51.830436Z","shell.execute_reply":"2024-10-13T17:51:24.152229Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting modal\n  Downloading modal-0.64.178-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from modal) (3.9.5)\nCollecting aiostream~=0.5.2 (from modal)\n  Downloading aiostream-0.5.2-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from modal) (2024.7.4)\nRequirement already satisfied: click>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from modal) (8.1.7)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from modal) (0.111.0)\nCollecting grpclib==0.4.7 (from modal)\n  Downloading grpclib-0.4.7.tar.gz (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: protobuf!=4.24.0,<5.0,>=3.19 in /opt/conda/lib/python3.10/site-packages (from modal) (3.20.3)\nRequirement already satisfied: rich>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from modal) (13.7.1)\nCollecting synchronicity~=0.8.2 (from modal)\n  Downloading synchronicity-0.8.2-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from modal) (0.10.2)\nRequirement already satisfied: typer>=0.9 in /opt/conda/lib/python3.10/site-packages (from modal) (0.12.3)\nCollecting types-certifi (from modal)\n  Downloading types_certifi-2021.10.8.3-py3-none-any.whl.metadata (1.4 kB)\nCollecting types-toml (from modal)\n  Downloading types_toml-0.10.8.20240310-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: watchfiles in /opt/conda/lib/python3.10/site-packages (from modal) (0.22.0)\nRequirement already satisfied: typing-extensions~=4.6 in /opt/conda/lib/python3.10/site-packages (from modal) (4.12.2)\nCollecting h2<5,>=3.1.0 (from grpclib==0.4.7->modal)\n  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: multidict in /opt/conda/lib/python3.10/site-packages (from grpclib==0.4.7->modal) (6.0.5)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=12.0.0->modal) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=12.0.0->modal) (2.18.0)\nCollecting sigtools==4.0.1 (from synchronicity~=0.8.2->modal)\n  Downloading sigtools-4.0.1-py2.py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: attrs in /opt/conda/lib/python3.10/site-packages (from sigtools==4.0.1->synchronicity~=0.8.2->modal) (23.2.0)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9->modal) (1.5.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->modal) (1.3.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->modal) (1.4.1)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->modal) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->modal) (4.0.3)\nRequirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->modal) (0.37.2)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from fastapi->modal) (2.8.2)\nRequirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->modal) (0.0.4)\nRequirement already satisfied: httpx>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->modal) (0.27.0)\nRequirement already satisfied: jinja2>=2.11.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->modal) (3.1.4)\nRequirement already satisfied: python-multipart>=0.0.7 in /opt/conda/lib/python3.10/site-packages (from fastapi->modal) (0.0.9)\nRequirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi->modal) (5.10.0)\nRequirement already satisfied: orjson>=3.2.1 in /opt/conda/lib/python3.10/site-packages (from fastapi->modal) (3.10.4)\nRequirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->modal) (2.1.1)\nRequirement already satisfied: uvicorn>=0.12.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->modal) (0.30.1)\nRequirement already satisfied: anyio>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from watchfiles->modal) (4.4.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.0.0->watchfiles->modal) (3.7)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.0.0->watchfiles->modal) (1.3.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.0.0->watchfiles->modal) (1.2.0)\nRequirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->modal) (2.6.1)\nCollecting hyperframe<7,>=6.0 (from h2<5,>=3.1.0->grpclib==0.4.7->modal)\n  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\nCollecting hpack<5,>=4.0 (from h2<5,>=3.1.0->grpclib==0.4.7->modal)\n  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi->modal) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.23.0->fastapi->modal) (0.14.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.2->fastapi->modal) (2.1.5)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->modal) (0.1.2)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->modal) (0.7.0)\nRequirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->modal) (2.20.1)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->modal) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->modal) (1.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->modal) (6.0.2)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->modal) (0.19.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->modal) (12.0)\nDownloading modal-0.64.178-py3-none-any.whl (552 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m552.6/552.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading aiostream-0.5.2-py3-none-any.whl (39 kB)\nDownloading synchronicity-0.8.2-py3-none-any.whl (31 kB)\nDownloading sigtools-4.0.1-py2.py3-none-any.whl (76 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading types_certifi-2021.10.8.3-py3-none-any.whl (2.1 kB)\nDownloading types_toml-0.10.8.20240310-py3-none-any.whl (4.8 kB)\nDownloading h2-4.1.0-py3-none-any.whl (57 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hpack-4.0.0-py3-none-any.whl (32 kB)\nDownloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\nBuilding wheels for collected packages: grpclib\n  Building wheel for grpclib (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for grpclib: filename=grpclib-0.4.7-py3-none-any.whl size=76219 sha256=7d24c14a4f903cd9f6a9317621c62ea7799b2eb1708c54d7f19d28696cf9bff1\n  Stored in directory: /root/.cache/pip/wheels/05/c0/1c/3d807409d0c67efeab2949832ba409205b1b6fe03f739ae4c1\nSuccessfully built grpclib\nInstalling collected packages: types-certifi, types-toml, sigtools, hyperframe, hpack, aiostream, synchronicity, h2, grpclib, modal\nSuccessfully installed aiostream-0.5.2 grpclib-0.4.7 h2-4.1.0 hpack-4.0.0 hyperframe-6.0.1 modal-0.64.178 sigtools-4.0.1 synchronicity-0.8.2 types-certifi-2021.10.8.3 types-toml-0.10.8.20240310\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:52:31.539467Z","iopub.execute_input":"2024-10-13T17:52:31.540303Z","iopub.status.idle":"2024-10-13T17:52:32.518753Z","shell.execute_reply.started":"2024-10-13T17:52:31.540259Z","shell.execute_reply":"2024-10-13T17:52:32.517458Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!modal token set --token-id ak-SeA6mwfp0axNqaR2IMEQH3 --token-secret as-2cut1gZPDlf5MWMnSV0F6S ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!modal token set --token-id ak-SeA6mwfp0axNqaR2IMEQH3 --token-secret as-2cut1gZPDlf5MWMnSV0F6S ","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:51:26.780545Z","iopub.execute_input":"2024-10-13T17:51:26.781544Z","iopub.status.idle":"2024-10-13T17:51:29.230384Z","shell.execute_reply.started":"2024-10-13T17:51:26.781490Z","shell.execute_reply":"2024-10-13T17:51:29.229132Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Verifying token against \u001b[4;34mhttps://api.modal.com\u001b[0m\n\u001b[32mToken verified successfully!\u001b[0m\n\u001b[?25l\u001b[32m⠋\u001b[0m Storing token\n\u001b[1A\u001b[2K\u001b[32mToken written to \u001b[0m\u001b[35m/root/\u001b[0m\u001b[35m.modal.toml\u001b[0m\u001b[32m in profile \u001b[0m\u001b[35mupproj\u001b[0m\u001b[32m.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!modal volume get my-volume checkpoint_epoch_0_batch_301799/","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:52:41.345876Z","iopub.execute_input":"2024-10-13T17:52:41.346262Z","iopub.status.idle":"2024-10-13T17:52:59.799384Z","shell.execute_reply.started":"2024-10-13T17:52:41.346227Z","shell.execute_reply":"2024-10-13T17:52:59.798411Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\u001b[?25l\u001b[34m⠋\u001b[0m Downloading file(s) to local...\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m Downloading file(s) to local...0\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1;37m.\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m Downloading file(s) to local...0\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1;37m.\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m Downloading file(s) to local...0\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1;37m.\u001b[0m\n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1;37m.\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/training_state.pt\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[35m0.0%\u001b[0m • \u001b[32m0.0…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[36m-:--:…\u001b[0m\n                                                               \u001b[32mMB  \u001b[0m             \n     \u001b[1;37mcheckpoint_epoch_0_batch_301799/scheduler.pt\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[35m0.0%\u001b[0m • \u001b[32m0.0…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[36m-:--:…\u001b[0m\n                                                               \u001b[32mkB  \u001b[0m             \n     \u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[35m0.0%\u001b[0m • \u001b[32m0.0…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[36m-:--:…\u001b[0m\n                                                               \u001b[32mGB  \u001b[0m             \n\u001b[1;37mcheckpoint_epoch_0_batch_301799/model.safetensors\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[35m0.0%\u001b[0m • \u001b[32m0.0…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[36m-:--:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m Downloading file(s) to local...\n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:01\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1;37m.\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/training_state.pt\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[35m0.0%\u001b[0m • \u001b[32m0.0…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[36m-:--:…\u001b[0m\n                                                               \u001b[32mMB  \u001b[0m             \n     \u001b[1;37mcheckpoint_epoch_0_batch_301799/scheduler.pt\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[35m0.0%\u001b[0m • \u001b[32m0.0…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[36m-:--:…\u001b[0m\n                                                               \u001b[32mkB  \u001b[0m             \n     \u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[35m0.0%\u001b[0m • \u001b[32m0.0…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[36m-:--:…\u001b[0m\n                                                               \u001b[32mGB  \u001b[0m             \n\u001b[1;37mcheckpoint_epoch_0_batch_301799/model.safetensors\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[35m0.0%\u001b[0m • \u001b[32m0.0…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[36m-:--:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m Downloading file(s) to local...\n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:01\u001b[0m \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[1;37m(1 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/training_state.pt\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[35m0.0%\u001b[0m • \u001b[32m0.0…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[36m-:--:…\u001b[0m\n                                                               \u001b[32mMB  \u001b[0m             \n     \u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[35m0.0%\u001b[0m • \u001b[32m0.0…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[36m-:--:…\u001b[0m\n                                                               \u001b[32mGB  \u001b[0m             \n\u001b[1;37mcheckpoint_epoch_0_batch_301799/model.safetensors\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[35m0.0%\u001b[0m • \u001b[32m0.0…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[36m-:--:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m Downloading file(s) to local...\n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:01\u001b[0m \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[1;37m(1 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/training_state.pt\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[35m0.0%\u001b[0m • \u001b[32m0.0…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[36m-:--:…\u001b[0m\n                                                               \u001b[32mMB  \u001b[0m             \n     \u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[35m0.0%\u001b[0m • \u001b[32m0.0…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[36m-:--:…\u001b[0m\n                                                               \u001b[32mGB  \u001b[0m             \n\u001b[1;37mcheckpoint_epoch_0_batch_301799/model.safetensors\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[35m0.0%\u001b[0m • \u001b[32m0.0…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[36m-:--:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m Downloading file(s) to local...\n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:01\u001b[0m \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[1;37m(2 out of 4 files completed)\u001b[0m\n     \u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[35m1.0%\u001b[0m • \u001b[32m0.0…\u001b[0m • \u001b[31m81.6\u001b[0m • \u001b[36m0:0…\u001b[0m\n                                                              \u001b[32mGB  \u001b[0m   \u001b[31mMB/s\u001b[0m       \n\u001b[1;37mcheckpoint_epoch_0_batch_301799/model.safetensors\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[35m0.0%\u001b[0m • \u001b[32m0.1…\u001b[0m • \u001b[31m4.1 \u001b[0m • \u001b[36m0:0…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m Downloading file(s) to local...s\u001b[0m       \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:02\u001b[0m \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[1;37m(2 out of 4 files completed)\u001b[0m\n     \u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[35m3.0%\u001b[0m  • \u001b[32m0.…\u001b[0m • \u001b[31m95.9\u001b[0m • \u001b[36m0:0…\u001b[0m\n                                                               \u001b[32mGB \u001b[0m   \u001b[31mMB/s\u001b[0m       \n\u001b[1;37mcheckpoint_epoch_0_batch_301799/model.safetensors\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[35m10.1%\u001b[0m • \u001b[32m33…\u001b[0m • \u001b[31m119…\u001b[0m • \u001b[36m0:0…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m Downloading file(s) to local...s\u001b[0m       \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:02\u001b[0m \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[1;37m(2 out of 4 files completed)\u001b[0m\n     \u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[35m5.0%\u001b[0m  • \u001b[32m0.…\u001b[0m • \u001b[31m97.8\u001b[0m • \u001b[36m0:0…\u001b[0m\n                                                               \u001b[32mGB \u001b[0m   \u001b[31mMB/s\u001b[0m       \n\u001b[1;37mcheckpoint_epoch_0_batch_301799/model.safetensors\u001b[0m \u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m21.5%\u001b[0m • \u001b[32m70…\u001b[0m • \u001b[31m131…\u001b[0m • \u001b[36m0:0…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m Downloading file(s) to local...s\u001b[0m       \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:02\u001b[0m \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[1;37m(2 out of 4 files completed)\u001b[0m\n     \u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[35m6.9%\u001b[0m  • \u001b[32m0.…\u001b[0m • \u001b[31m100…\u001b[0m • \u001b[36m0:0…\u001b[0m\n                                                               \u001b[32mGB \u001b[0m   \u001b[31mMB/s\u001b[0m       \n\u001b[1;37mcheckpoint_epoch_0_batch_301799/model.safetensors\u001b[0m \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[35m33.8%\u001b[0m • \u001b[32m11…\u001b[0m • \u001b[31m140…\u001b[0m • \u001b[36m0:0…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m Downloading file(s) to local...s\u001b[0m       \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:02\u001b[0m \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[1;37m(2 out of 4 files completed)\u001b[0m\n     \u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[35m8.7%\u001b[0m  • \u001b[32m0.…\u001b[0m • \u001b[31m99.0\u001b[0m • \u001b[36m0:0…\u001b[0m\n                                                               \u001b[32mGB \u001b[0m   \u001b[31mMB/s\u001b[0m       \n\u001b[1;37mcheckpoint_epoch_0_batch_301799/model.safetensors\u001b[0m \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[35m45.9%\u001b[0m • \u001b[32m15…\u001b[0m • \u001b[31m143…\u001b[0m • \u001b[36m0:0…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m Downloading file(s) to local...s\u001b[0m       \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:03\u001b[0m \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[1;37m(2 out of 4 files completed)\u001b[0m\n     \u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[35m10.3%\u001b[0m • \u001b[32m0.…\u001b[0m • \u001b[31m94.1\u001b[0m • \u001b[36m0:0…\u001b[0m\n                                                               \u001b[32mGB \u001b[0m   \u001b[31mMB/s\u001b[0m       \n\u001b[1;37mcheckpoint_epoch_0_batch_301799/model.safetensors\u001b[0m \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[35m55.3%\u001b[0m • \u001b[32m18…\u001b[0m • \u001b[31m145…\u001b[0m • \u001b[36m0:0…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m Downloading file(s) to local...s\u001b[0m       \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:03\u001b[0m \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[1;37m(2 out of 4 files completed)\u001b[0m\n     \u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[35m12.2%\u001b[0m • \u001b[32m0.…\u001b[0m • \u001b[31m93.7\u001b[0m • \u001b[36m0:0…\u001b[0m\n                                                               \u001b[32mGB \u001b[0m   \u001b[31mMB/s\u001b[0m       \n\u001b[1;37mcheckpoint_epoch_0_batch_301799/model.safetensors\u001b[0m \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[35m63.6%\u001b[0m • \u001b[32m20…\u001b[0m • \u001b[31m135…\u001b[0m • \u001b[36m0:0…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m Downloading file(s) to local...s\u001b[0m       \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:03\u001b[0m \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[1;37m(2 out of 4 files completed)\u001b[0m\n     \u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m14.0%\u001b[0m • \u001b[32m0.…\u001b[0m • \u001b[31m92.9\u001b[0m • \u001b[36m0:0…\u001b[0m\n                                                               \u001b[32mGB \u001b[0m   \u001b[31mMB/s\u001b[0m       \n\u001b[1;37mcheckpoint_epoch_0_batch_301799/model.safetensors\u001b[0m \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[35m74.2%\u001b[0m • \u001b[32m24…\u001b[0m • \u001b[31m131…\u001b[0m • \u001b[36m0:0…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m Downloading file(s) to local...s\u001b[0m       \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:03\u001b[0m \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[1;37m(2 out of 4 files completed)\u001b[0m\n     \u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m15.4%\u001b[0m • \u001b[32m0.…\u001b[0m • \u001b[31m88.7\u001b[0m • \u001b[36m0:0…\u001b[0m\n                                                               \u001b[32mGB \u001b[0m   \u001b[31mMB/s\u001b[0m       \n\u001b[1;37mcheckpoint_epoch_0_batch_301799/model.safetensors\u001b[0m \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[35m85.1%\u001b[0m • \u001b[32m28…\u001b[0m • \u001b[31m126…\u001b[0m • \u001b[36m0:0…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m Downloading file(s) to local...s\u001b[0m       \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:04\u001b[0m \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[1;37m(2 out of 4 files completed)\u001b[0m\n     \u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m17.6%\u001b[0m • \u001b[32m0.…\u001b[0m • \u001b[31m91.7\u001b[0m • \u001b[36m0:0…\u001b[0m\n                                                               \u001b[32mGB \u001b[0m   \u001b[31mMB/s\u001b[0m       \n\u001b[1;37mcheckpoint_epoch_0_batch_301799/model.safetensors\u001b[0m \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[35m93.7%\u001b[0m • \u001b[32m30…\u001b[0m • \u001b[31m129…\u001b[0m • \u001b[36m0:0…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m Downloading file(s) to local...s\u001b[0m       \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:04\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[35m19.5%\u001b[0m • \u001b[32m0.3/…\u001b[0m • \u001b[31m94.5\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:04\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m20.8%\u001b[0m • \u001b[32m0.3/…\u001b[0m • \u001b[31m93.5\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:04\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m22.9%\u001b[0m • \u001b[32m0.3/…\u001b[0m • \u001b[31m87.2\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:05\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m24.7%\u001b[0m • \u001b[32m0.3/…\u001b[0m • \u001b[31m99.6\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:05\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m26.4%\u001b[0m • \u001b[32m0.3/…\u001b[0m • \u001b[31m91.2\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:05\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m28.4%\u001b[0m • \u001b[32m0.4/…\u001b[0m • \u001b[31m100…\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:05\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m29.9%\u001b[0m • \u001b[32m0.4/…\u001b[0m • \u001b[31m92.8\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:06\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m31.8%\u001b[0m • \u001b[32m0.4/…\u001b[0m • \u001b[31m86.8\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:06\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m33.4%\u001b[0m • \u001b[32m0.4/…\u001b[0m • \u001b[31m93.3\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:06\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m35.3%\u001b[0m • \u001b[32m0.5/…\u001b[0m • \u001b[31m89.3\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:06\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m36.8%\u001b[0m • \u001b[32m0.5/…\u001b[0m • \u001b[31m84.3\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:07\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m38.5%\u001b[0m • \u001b[32m0.5/…\u001b[0m • \u001b[31m84.4\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:07\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[35m40.1%\u001b[0m • \u001b[32m0.5/…\u001b[0m • \u001b[31m87.1\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:07\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[35m41.3%\u001b[0m • \u001b[32m0.5/…\u001b[0m • \u001b[31m75.6\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:07\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[35m43.1%\u001b[0m • \u001b[32m0.6/…\u001b[0m • \u001b[31m86.2\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:08\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[35m44.7%\u001b[0m • \u001b[32m0.6/…\u001b[0m • \u001b[31m87.1\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:08\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[35m46.7%\u001b[0m • \u001b[32m0.6/…\u001b[0m • \u001b[31m92.3\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:08\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[35m48.4%\u001b[0m • \u001b[32m0.6/…\u001b[0m • \u001b[31m92.2\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:08\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[35m50.4%\u001b[0m • \u001b[32m0.7/…\u001b[0m • \u001b[31m97.1\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:09\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[35m51.8%\u001b[0m • \u001b[32m0.7/…\u001b[0m • \u001b[31m89.7\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:09\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[35m53.3%\u001b[0m • \u001b[32m0.7/…\u001b[0m • \u001b[31m84.3\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:09\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[35m55.2%\u001b[0m • \u001b[32m0.7/…\u001b[0m • \u001b[31m82.1\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:09\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[35m57.1%\u001b[0m • \u001b[32m0.8/…\u001b[0m • \u001b[31m91.9\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:10\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[35m58.9%\u001b[0m • \u001b[32m0.8/…\u001b[0m • \u001b[31m99.5\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:10\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[35m60.4%\u001b[0m • \u001b[32m0.8/…\u001b[0m • \u001b[31m87.6\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:10\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[35m61.9%\u001b[0m • \u001b[32m0.8/…\u001b[0m • \u001b[31m79.8\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:11\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[35m63.6%\u001b[0m • \u001b[32m0.8/…\u001b[0m • \u001b[31m87.0\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:11\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[35m65.5%\u001b[0m • \u001b[32m0.9/…\u001b[0m • \u001b[31m91.2\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:11\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[35m66.8%\u001b[0m • \u001b[32m0.9/…\u001b[0m • \u001b[31m87.1\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:11\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[35m68.3%\u001b[0m • \u001b[32m0.9/…\u001b[0m • \u001b[31m79.0\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:12\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[35m70.2%\u001b[0m • \u001b[32m0.9/…\u001b[0m • \u001b[31m81.2\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:12\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[35m72.0%\u001b[0m • \u001b[32m0.9/…\u001b[0m • \u001b[31m88.8\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:12\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[35m74.0%\u001b[0m • \u001b[32m1.0/…\u001b[0m • \u001b[31m104…\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:12\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[35m75.7%\u001b[0m • \u001b[32m1.0/…\u001b[0m • \u001b[31m96.2\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:13\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[35m77.7%\u001b[0m • \u001b[32m1.0/…\u001b[0m • \u001b[31m96.8\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:13\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[35m79.4%\u001b[0m • \u001b[32m1.0/…\u001b[0m • \u001b[31m94.6\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:13\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[35m81.0%\u001b[0m • \u001b[32m1.1/…\u001b[0m • \u001b[31m89.8\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:13\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[35m82.8%\u001b[0m • \u001b[32m1.1/…\u001b[0m • \u001b[31m89.7\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:14\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[35m84.6%\u001b[0m • \u001b[32m1.1/…\u001b[0m • \u001b[31m89.4\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:14\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[35m87.0%\u001b[0m • \u001b[32m1.1/…\u001b[0m • \u001b[31m113…\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:14\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[35m88.8%\u001b[0m • \u001b[32m1.2/…\u001b[0m • \u001b[31m104…\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:14\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[35m90.5%\u001b[0m • \u001b[32m1.2/…\u001b[0m • \u001b[31m93.6\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:15\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[35m92.3%\u001b[0m • \u001b[32m1.2/…\u001b[0m • \u001b[31m88.2\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:15\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[35m94.1%\u001b[0m • \u001b[32m1.2/…\u001b[0m • \u001b[31m93.9\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:15\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[35m95.8%\u001b[0m • \u001b[32m1.3/…\u001b[0m • \u001b[31m90.1\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:15\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[35m97.6%\u001b[0m • \u001b[32m1.3/…\u001b[0m • \u001b[31m92.8\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:16\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[35m98.9%\u001b[0m • \u001b[32m1.3/…\u001b[0m • \u001b[31m80.8\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m Downloading file(s) to local...[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1;37mDownloading file(s) to local...\u001b[0m \u001b[33m0:00:16\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[1;37m(3 out of 4 files completed)\u001b[0m\n\u001b[1;37mcheckpoint_epoch_0_batch_301799/optimizer.pt\u001b[0m \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[35m99.9%\u001b[0m • \u001b[32m1.3/…\u001b[0m • \u001b[31m87.4\u001b[0m • \u001b[36m0:00:…\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m Post processing...  \u001b[32mGB   \u001b[0m   \u001b[31mMB/s\u001b[0m         \n\u001b[1A\u001b[2K\u001b[32m✓\u001b[0m Finished downloading files to local!\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Define the content to be written\ncontent = r'''\nimport modal\nimport os\nimport torch\nimport torch.optim as optim\nfrom torch.utils.cpp_extension import load\nfrom tqdm import tqdm\nimport math\nfrom transformers import GPT2Tokenizer\nfrom datasets import load_dataset\n\ncuda_version = \"12.4.0\"  # should be no greater than host CUDA version\nflavor = \"devel\"  # includes full CUDA toolkit\nos_version = \"ubuntu22.04\"\ntag = f\"{cuda_version}-{flavor}-{os_version}\"\nvolume = modal.Volume.from_name(\"my-volume\", create_if_missing=True)\n\n# Define the Modal stub and image\napp = modal.App(\"complex_model_stub\")\nimage = modal.Image.debian_slim().from_registry(f\"nvidia/cuda:{tag}\", add_python=\"3.11\").pip_install(\n    \"torch\", \"transformers\", \"datasets==2.20.0\", \"einops\", \"tqdm\", \"ninja\", \"packaging\", \"wheel\", \"zstandard\", \"numba\", \"textstat\"\n)\n\n# Specify GPU, custom image, and volume mount requirements\n@app.function(image=image, gpu=\"t4\", timeout=86400, volumes={\"/checkpoints\": volume})\nasync def run_training():\n    import torch\n    from numba import cuda, float64\n    import math\n    import numpy as np\n    import textstat\n    import torch\n    import torch.nn.functional as F\n    from torch.distributions import Categorical\n\n    # Ensure that Numba uses the same device as PyTorch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # -------------------------------\n    # CUDA Kernels\n    # -------------------------------\n\n    @cuda.jit\n    def fused_scan_fwd_kernel(u, delta, A, B, C, D, output, b, l, d_in, n):\n        \"\"\"\n        Forward pass kernel implementing Blelloch's inclusive scan.\n        \"\"\"\n        shared_mem = cuda.shared.array(shape=512, dtype=float64)\n        input_x = shared_mem[:256]\n        scan_x = shared_mem[256:]\n\n        idx = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n        total = b * l * d_in\n\n        if idx >= total:\n            return\n\n        dim = idx % d_in\n        seq = (idx // d_in) % l\n        batch = idx // (l * d_in)\n\n        u_val = u[batch, seq, dim]\n        delta_val = delta[batch, seq, dim]\n\n        # Initialize scan_x to zero\n        for j in range(n):\n            if j < n:\n                scan_x[j] = 0.0\n        cuda.syncthreads()\n\n        # Compute scan_x[j] = exp(delta * A[j]) * scan_x[j] + delta * B[j] * u\n        for j in range(n):\n            if j < n:\n                scan_x[j] = math.exp(delta_val * A[dim, j]) * scan_x[j] + delta_val * B[batch, seq, j] * u_val\n        cuda.syncthreads()\n\n        # Store original scan_x before scan for inclusive scan\n        for j in range(n):\n            if j < n:\n                input_x[j] = scan_x[j]\n        cuda.syncthreads()\n\n        # Perform Blelloch's exclusive scan on scan_x\n        step = 1\n        while step < n:\n            index = 2 * step * cuda.threadIdx.x + (2 * step - 1)\n            if index < n:\n                scan_x[index] += scan_x[index - step]\n            cuda.syncthreads()\n            step *= 2\n\n        if cuda.threadIdx.x == 0 and n > 0:\n            scan_x[n - 1] = 0.0\n        cuda.syncthreads()\n\n        step = n // 2\n        while step >= 1:\n            index = 2 * step * cuda.threadIdx.x + (2 * step - 1)\n            if index < n:\n                t = scan_x[index - step]\n                scan_x[index - step] = scan_x[index]\n                scan_x[index] += t\n            cuda.syncthreads()\n            step //= 2\n\n        # Inclusive scan: add input_x[j] to scan_x[j]\n        for j in range(n):\n            if j < n:\n                scan_x[j] += input_x[j]\n        cuda.syncthreads()\n\n        # Now scan_x contains the inclusive scan result\n        # Compute y = sum(scan_x[j] * C[j]) + u * D[dim]\n        y = 0.0\n        for j in range(n):\n            if j < n:\n                y += scan_x[j] * C[batch, seq, j]\n        y += u_val * D[dim]\n\n        output[batch, seq, dim] = y\n\n\n\n    @cuda.jit\n    def fused_scan_bwd_kernel(u, delta, A, B, C, D, grad_output, grad_u, grad_delta,\n                              grad_A, grad_B, grad_C, grad_D, b, l, d_in, n):\n        \"\"\"\n        Backward pass kernel implementing Blelloch's inclusive scan and reverse scan for gradient computations.\n        \"\"\"\n        # Allocate shared memory: double the size for input and scan\n        shared_mem = cuda.shared.array(shape=1024, dtype=float64)  # Adjust size if needed\n        input_x = shared_mem[:256]\n        scan_x = shared_mem[256:512]\n        grad_shared_A = shared_mem[512:768]\n        grad_shared_B = shared_mem[768:1024]\n\n        # Initialize shared gradients to zero\n        for j in range(n):\n            if j < n:\n                grad_shared_A[j] = 0.0\n                grad_shared_B[j] = 0.0\n        cuda.syncthreads()\n\n        idx = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n        total = b * l * d_in\n\n        if idx >= total:\n            return\n\n        dim = idx % d_in\n        seq = (idx // d_in) % l\n        batch = idx // (l * d_in)\n\n        u_val = u[batch, seq, dim]\n        delta_val = delta[batch, seq, dim]\n\n        y_grad = grad_output[batch, seq, dim]\n\n        # Initialize scan_x to zero\n        for j in range(n):\n            if j < n:\n                scan_x[j] = 0.0\n        cuda.syncthreads()\n\n        # Recompute scan_x as in forward pass\n        for j in range(n):\n            if j < n:\n                scan_x[j] = math.exp(delta_val * A[dim, j]) * scan_x[j] + delta_val * B[batch, seq, j] * u_val\n        cuda.syncthreads()\n\n        # Store original scan_x before scan for inclusive scan\n        for j in range(n):\n            if j < n:\n                input_x[j] = scan_x[j]\n        cuda.syncthreads()\n\n        # Perform Blelloch's exclusive scan on scan_x\n        step = 1\n        while step < n:\n            index = 2 * step * cuda.threadIdx.x + (2 * step - 1)\n            if index < n:\n                scan_x[index] += scan_x[index - step]\n            cuda.syncthreads()\n            step *= 2\n\n        if cuda.threadIdx.x == 0 and n > 0:\n            scan_x[n - 1] = 0.0\n        cuda.syncthreads()\n\n        step = n // 2\n        while step >= 1:\n            index = 2 * step * cuda.threadIdx.x + (2 * step - 1)\n            if index < n:\n                t = scan_x[index - step]\n                scan_x[index - step] = scan_x[index]\n                scan_x[index] += t\n            cuda.syncthreads()\n            step //= 2\n\n        # Inclusive scan: add input_x[j] to scan_x[j]\n        for j in range(n):\n            if j < n:\n                scan_x[j] += input_x[j]\n        cuda.syncthreads()\n\n        # Reverse scan to propagate gradients\n\n        # Initialize gradient accumulation for A and B\n        for j in range(n):\n            if j < n:\n                grad_shared_A[j] = 0.0\n                grad_shared_B[j] = 0.0\n        cuda.syncthreads()\n\n        # Compute gradients\n        for j in range(n):\n            if j < n:\n                # Gradient w.r.t C[j]\n                grad_C_val = scan_x[j] * y_grad\n                grad_C[batch, seq, j] += grad_C_val\n\n                # Gradient w.r.t A[j]\n                grad_A_val = delta_val * math.exp(delta_val * A[dim, j]) * scan_x[j] * y_grad\n                grad_shared_A[j] += grad_A_val\n\n                # Gradient w.r.t B[j]\n                grad_B_val = delta_val * u_val * y_grad\n                grad_shared_B[j] += grad_B_val\n\n                # Gradient w.r.t D[dim]\n                grad_D[dim] += u_val * y_grad\n\n        # Now, accumulate shared gradients for A and B\n        for j in range(n):\n            if j < n:\n                A_grad = grad_shared_A[j]\n                B_grad = grad_shared_B[j]\n                grad_A[dim, j] += A_grad\n                grad_B[batch, seq, j] += B_grad\n\n        # Gradient w.r.t delta\n        for j in range(n):\n            if j < n:\n                grad_delta_val = y_grad * (A[dim, j] * math.exp(delta_val * A[dim, j]) * scan_x[j] +\n                                            B[batch, seq, j] * u_val)\n                grad_delta[batch, seq, dim] += grad_delta_val\n\n        # Gradient w.r.t u\n        grad_u_val = 0.0\n        for j in range(n):\n            if j < n:\n                grad_u_val += delta_val * B[batch, seq, j] * C[batch, seq, j] * y_grad\n        grad_u[batch, seq, dim] += grad_u_val\n\n\n    # -------------------------------\n    # PyTorch Autograd Function\n    # -------------------------------\n\n    class FusedScanFunction(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, u, delta, A, B, C, D):\n            \"\"\"\n            Forward pass using the custom CUDA kernel.\n            \"\"\"\n            # Detach inputs to prevent PyTorch from tracking gradients through them\n            u_detached = u.detach().contiguous().double().to(device)\n            delta_detached = delta.detach().contiguous().double().to(device)\n            A_detached = A.detach().contiguous().double().to(device)\n            B_detached = B.detach().contiguous().double().to(device)\n            C_detached = C.detach().contiguous().double().to(device)\n            D_detached = D.detach().contiguous().double().to(device)\n\n            b, l, d_in = u_detached.shape\n            n = A_detached.shape[1]\n\n            # Allocate output tensor\n            output = torch.zeros_like(u_detached, device=device, dtype=torch.float64)\n\n            # Define grid and block dimensions\n            threads_per_block = 256\n            blocks_per_grid = (b * l * d_in + (threads_per_block - 1)) // threads_per_block\n\n            # Launch the forward kernel\n            fused_scan_fwd_kernel[blocks_per_grid, threads_per_block](\n                cuda.as_cuda_array(u_detached),\n                cuda.as_cuda_array(delta_detached),\n                cuda.as_cuda_array(A_detached),\n                cuda.as_cuda_array(B_detached),\n                cuda.as_cuda_array(C_detached),\n                cuda.as_cuda_array(D_detached),\n                cuda.as_cuda_array(output),\n                b, l, d_in, n\n            )\n\n            # Save context for backward\n            ctx.save_for_backward(u_detached, delta_detached, A_detached, B_detached, C_detached, D_detached)\n            ctx.n = n\n            ctx.b = b\n            ctx.l = l\n            ctx.d_in = d_in\n\n            return output\n\n        @staticmethod\n        def backward(ctx, grad_output):\n            u_detached, delta_detached, A_detached, B_detached, C_detached, D_detached = ctx.saved_tensors\n            n = ctx.n\n            b = ctx.b\n            l = ctx.l\n            d_in = ctx.d_in\n\n            grad_output_detached = grad_output.contiguous().double().to(device)\n\n            # Allocate gradient tensors\n            grad_u = torch.zeros_like(u_detached, device=device, dtype=torch.float64)\n            grad_delta = torch.zeros_like(delta_detached, device=device, dtype=torch.float64)\n            grad_A = torch.zeros_like(A_detached, device=device, dtype=torch.float64)\n            grad_B = torch.zeros_like(B_detached, device=device, dtype=torch.float64)\n            grad_C = torch.zeros_like(C_detached, device=device, dtype=torch.float64)\n            grad_D = torch.zeros_like(D_detached, device=device, dtype=torch.float64)\n\n            # Define grid and block dimensions\n            threads_per_block = 256\n            blocks_per_grid = (b * l * d_in + (threads_per_block - 1)) // threads_per_block\n\n            # Launch the backward kernel\n            fused_scan_bwd_kernel[blocks_per_grid, threads_per_block](\n                cuda.as_cuda_array(u_detached),\n                cuda.as_cuda_array(delta_detached),\n                cuda.as_cuda_array(A_detached),\n                cuda.as_cuda_array(B_detached),\n                cuda.as_cuda_array(C_detached),\n                cuda.as_cuda_array(D_detached),\n                cuda.as_cuda_array(grad_output_detached),\n                cuda.as_cuda_array(grad_u),\n                cuda.as_cuda_array(grad_delta),\n                cuda.as_cuda_array(grad_A),\n                cuda.as_cuda_array(grad_B),\n                cuda.as_cuda_array(grad_C),\n                cuda.as_cuda_array(grad_D),\n                b, l, d_in, n\n            )\n\n            return grad_u, grad_delta, grad_A, grad_B, grad_C, grad_D\n\n\n    # -------------------------------\n    # Custom PyTorch Module\n    # -------------------------------\n\n\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    import torch.nn.functional as F\n    from torch.autograd import Function\n    from torch.utils.checkpoint import checkpoint\n    import numpy as np\n    from numba import cuda\n    import math\n    from einops import repeat\n    import sys\n    from torch.optim.lr_scheduler import CosineAnnealingLR\n    import os\n    import math\n    import random\n    import shutil\n    import re\n    from collections import Counter\n\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.cpp_extension import load\n    from torch.utils.checkpoint import checkpoint\n    from safetensors.torch import save_model, load_model\n    from torch.nn.utils.rnn import pad_sequence\n    from torch.optim import Optimizer\n    from torch.optim.lr_scheduler import CosineAnnealingLR\n    from torch.autograd import Function\n    from einops import repeat\n    from transformers import GPT2Tokenizer\n    from datasets import load_dataset\n    from tqdm import tqdm\n\n    # ============================\n    # CUDA Kernels\n    # ============================\n\n    # Wrapper function to launch the forward kernel\n    def fused_scan_fwd(u, delta, A, B, C, D):\n        b, l, d_in = u.shape\n        n = A.shape[1]\n        output = cuda.device_array_like(u)\n\n        threads_per_block = 256\n        blocks_per_grid = (b * l * d_in + threads_per_block - 1) // threads_per_block\n        blocks_per_grid = max(blocks_per_grid, 2)  # You can adjust this based on performance\n        # shared_mem_size is not needed since we're using fixed shared memory\n\n        fused_scan_fwd_kernel[blocks_per_grid, threads_per_block](\n            u, delta, A, B, C, D, output, b, l, d_in, n\n        )\n        return output.copy_to_host()\n\n    # Wrapper function to launch the backward kernel\n    def fused_scan_bwd(u, delta, A, B, C, D, grad_output):\n        b, l, d_in = u.shape\n        n = A.shape[1]\n        grad_u = cuda.device_array_like(u)\n        grad_delta = cuda.device_array_like(delta)\n        grad_A = cuda.device_array_like(A)\n        grad_B = cuda.device_array_like(B)\n        grad_C = cuda.device_array_like(C)\n        grad_D = cuda.device_array_like(D)\n\n        threads_per_block = 256\n        blocks_per_grid = (b * l * d_in + threads_per_block - 1) // threads_per_block\n        blocks_per_grid = max(blocks_per_grid, 2)  # You can adjust this based on performance\n        # shared_mem_size is not needed since we're using fixed shared memory\n\n        fused_scan_bwd_kernel[blocks_per_grid, threads_per_block](\n            u, delta, A, B, C, D, grad_output, grad_u, grad_delta, grad_A, grad_B, grad_C, grad_D,\n            b, l, d_in, n\n        )\n        return grad_u.copy_to_host(), grad_delta.copy_to_host(), grad_A.copy_to_host(), grad_B.copy_to_host(), grad_C.copy_to_host(), grad_D.copy_to_host()\n\n    # ============================\n    # Custom Layers and Functions\n    # ============================\n\n    def compute_dropout_rate(batch_idx, total_batches, max_dropout=0.4):\n        \"\"\"Compute dropout rate linearly increasing from 0 to max_dropout.\"\"\"\n        return min(max_dropout * (batch_idx / total_batches), max_dropout)\n\n\n    class RMSNorm(nn.Module):\n        def __init__(self, d_model: int, eps: float = 1e-5, use_mup: bool = False):\n            super().__init__()\n\n            self.use_mup = use_mup\n            self.eps = eps\n\n            # RMSNorm gains prevent muTransfer (section 4.2.3)\n            if not use_mup:\n                self.weight = nn.Parameter(torch.ones(d_model))\n\n        def forward(self, x):\n            output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n            if not self.use_mup:\n                return output * self.weight\n            else:\n                return output\n\n\n    class SelectiveScanFunction(Function):\n        @staticmethod\n        def forward(ctx, u, delta, A_log, B, C, D):\n            A = -torch.exp(A_log.float())\n            # Ensure all tensors are contiguous and on the correct device\n            u_contig = u.contiguous()\n            delta_contig = delta.contiguous()\n            A_contig = A.contiguous()\n            B_contig = B.contiguous()\n            C_contig = C.contiguous()\n            D_contig = D.contiguous()\n\n            # Move to Numba-compatible device arrays\n            u_device = cuda.to_device(u_contig.cpu().numpy())\n            delta_device = cuda.to_device(delta_contig.cpu().numpy())\n            A_device = cuda.to_device(A_contig.cpu().numpy())\n            B_device = cuda.to_device(B_contig.cpu().numpy())\n            C_device = cuda.to_device(C_contig.cpu().numpy())\n            D_device = cuda.to_device(D_contig.cpu().numpy())\n\n            # Launch the forward kernel\n            output_host = fused_scan_fwd(u_device, delta_device, A_device, B_device, C_device, D_device)\n\n            # Convert output back to torch tensor\n            output = torch.from_numpy(output_host).to(u.device)\n\n            ctx.save_for_backward(u, delta, A_log, B, C, D)\n            return output\n\n        @staticmethod\n        def backward(ctx, grad_output):\n            u, delta, A_log, B, C, D = ctx.saved_tensors\n            A = -torch.exp(A_log)\n\n            # Move tensors to CPU and convert to NumPy\n            u_cpu = u.contiguous().cpu().numpy()\n            delta_cpu = delta.contiguous().cpu().numpy()\n            A_cpu = A.contiguous().cpu().numpy()\n            B_cpu = B.contiguous().cpu().numpy()\n            C_cpu = C.contiguous().cpu().numpy()\n            D_cpu = D.contiguous().cpu().numpy()\n            grad_output_cpu = grad_output.contiguous().cpu().numpy()\n\n            # Move to Numba-compatible device arrays\n            u_device = cuda.to_device(u_cpu)\n            delta_device = cuda.to_device(delta_cpu)\n            A_device = cuda.to_device(A_cpu)\n            B_device = cuda.to_device(B_cpu)\n            C_device = cuda.to_device(C_cpu)\n            D_device = cuda.to_device(D_cpu)\n            grad_output_device = cuda.to_device(grad_output_cpu)\n\n            # Launch the backward kernel\n            grad_u_host, grad_delta_host, grad_A_host, grad_B_host, grad_C_host, grad_D_host = fused_scan_bwd(\n                u_device, delta_device, A_device, B_device, C_device, D_device, grad_output_device\n            )\n\n            # Convert gradients back to torch tensors\n            grad_u = torch.from_numpy(grad_u_host).to(u.device)\n            grad_delta = torch.from_numpy(grad_delta_host).to(delta.device)\n            grad_A = torch.from_numpy(grad_A_host).to(A.device)\n            grad_B = torch.from_numpy(grad_B_host).to(B.device)\n            grad_C = torch.from_numpy(grad_C_host).to(C.device)\n            grad_D = torch.from_numpy(grad_D_host).to(D.device)\n\n            # Compute grad_A_log\n            grad_A_log = grad_A * (-torch.exp(A_log))\n\n            return grad_u, grad_delta, grad_A_log, grad_B, grad_C, grad_D\n\n\n    class SelectiveScanModel(nn.Module):\n        def __init__(self, d_in, d_state, dt_rank):\n            super().__init__()\n            self.norm = RMSNorm(d_in)\n            self.d_state = d_state\n            self.dt_rank = dt_rank\n            A = repeat(\n                torch.arange(1, self.d_state + 1, dtype=torch.float32),\n                \"n -> d n\",\n                d=d_in,\n            ).contiguous()\n            A_log = torch.log(A)\n            self.A_log = nn.Parameter(A_log)\n            self.D = nn.Parameter(torch.ones(d_in).cuda())\n            self.x_proj = nn.Linear(d_in, dt_rank + d_state * 2, bias=True).cuda()\n            self.dt_proj = nn.Linear(dt_rank, d_in, bias=True).cuda()\n            self.output_layer = nn.Linear(d_in, d_in).cuda()\n\n        def forward(self, x):\n            b, l, d_in = x.shape\n            D = self.D\n            x_dbl = self.x_proj(x)\n            delta, B, C = x_dbl.split([self.dt_rank, self.d_state, self.d_state], dim=-1)\n            delta = F.softplus(self.dt_proj(delta))\n            y = self.norm(SelectiveScanFunction.apply(x, delta, self.A_log, B, C, D))\n            return self.output_layer(y)\n\n\n    class MambaBlock(nn.Module):\n        def __init__(self, d_model, d_state=16, dt_rank=16, d_conv=4, expand=2, conv_bias=True, bias=False):\n            super().__init__()\n            self.d_model = d_model\n            self.d_state = d_state\n            self.d_conv = d_conv\n            self.expand = expand\n            self.d_inner = int(self.expand * self.d_model)\n            self.dt_rank = dt_rank\n            self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias)\n            self.conv1d = nn.Sequential(\n                nn.Conv1d(in_channels=self.d_inner, out_channels=self.d_inner, kernel_size=d_conv, groups=self.d_inner, bias=conv_bias),\n                nn.Conv1d(in_channels=self.d_inner, out_channels=self.d_inner, kernel_size=1, bias=conv_bias)\n            )\n            self.activation = nn.SiLU()\n            self.ssm_model = SelectiveScanModel(self.d_inner, self.d_state, self.dt_rank)\n            self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias)\n            self.norm = RMSNorm(self.d_model)\n        def forward(self, x):\n            b, l, d = x.shape\n            x_proj = self.in_proj(x)\n            x, res = x_proj.chunk(2, dim=-1)\n            x = x.transpose(1, 2)\n            x = F.pad(x, (self.d_conv - 1, 0))\n            x = self.conv1d(x)[:, :, :l]\n            x = x.transpose(1, 2)\n            x = self.activation(x)\n            y = self.ssm_model(x)\n            y = y * self.activation(res)\n            y = self.norm(self.out_proj(y))\n            return y\n\n\n    class BlockStack(nn.Module):\n        def __init__(self, num_blocks, d_model, d_state, dt_rank):\n            super().__init__()\n            self.blocks = nn.ModuleList([MambaBlock(d_model, d_state, dt_rank) for _ in range(num_blocks)])\n\n        def forward(self, x, dropout_rate=None):\n            x_res = x\n            for i in range(len(self.blocks)):\n                x = self.blocks[i](x)\n            return x * x_res\n\n\n    class FeedForwardLayer(nn.Module):\n        def __init__(self, embed_dim, d_model, hidden_dim, window_size=4):\n            \"\"\"\n            Initializes the FeedForwardLayer with windowed masking.\n\n            Args:\n                d_model (int): Dimension of the input and output features.\n                hidden_dim (int): Dimension of the hidden layer.\n                window_size (int): Size of the window around each position.\n            \"\"\"\n            super().__init__()\n            self.fc1 = nn.Linear(embed_dim, hidden_dim)\n            self.fc2 = nn.Linear(hidden_dim, d_model)\n            self.activation = nn.SiLU()\n            self.norm = RMSNorm(d_model)\n\n        def forward(self, x):\n            \"\"\"\n            Forward pass of the FeedForwardLayer with windowed masking.\n\n            Args:\n                x (torch.Tensor): Input tensor of shape (batch_size, seq_length, d_model).\n\n            Returns:\n                torch.Tensor: Output tensor of shape (batch_size, seq_length, d_model).\n            \"\"\"\n\n            # Manually apply the windowed mask to the weights of the first linear layer\n            masked_weight = self.fc1.weight  # Element-wise multiplication\n            masked_bias = self.fc1.bias\n\n            # Perform the masked linear transformation\n            x = torch.matmul(x, masked_weight.t()) + masked_bias\n\n            x = self.activation(x)\n            x = self.fc2(x)\n            x = self.norm(x)\n            return x\n    class HeadModel(nn.Module):\n        def __init__(self, vocab_size, embed_dim, d_state, dt_rank, d_model, num_blocks=1, num_matrices=2):\n            super().__init__()\n            self.embedding = nn.Embedding(vocab_size, embed_dim)\n            self.block_stacks = nn.ModuleList([\n                BlockStack(num_blocks, embed_dim // num_matrices, d_state, dt_rank)\n                for _ in range(num_matrices)\n            ])\n            self.activation = nn.SiLU()\n            self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n            self.policy_head = nn.Linear(d_model, 1, bias=False)  # New head for policy output (scalar for each token)\n            self.norm = nn.LayerNorm(d_model)\n            hidden_dim = d_model * 2\n            self.feed_forward = FeedForwardLayer(embed_dim, d_model, hidden_dim)\n            self.num_matrices = num_matrices\n    \n        def forward(self, input_ids, dropout_rate=None):\n            x = self.embedding(input_ids)\n            e = x\n            split_dim = x.shape[2] // self.num_matrices\n            x_splits = torch.split(x, split_dim, dim=2)\n            processed_splits = []\n            for i, feature_matrix in enumerate(x_splits):\n                block_stack = self.block_stacks[i]\n                processed_matrix = checkpoint(block_stack, feature_matrix, use_reentrant=False)\n                processed_splits.append(processed_matrix)\n            x = torch.cat(processed_splits, dim=2)\n            x = self.feed_forward(x + e)\n            x = self.norm(x)\n            \n            logits = self.lm_head(x)  # Output for language modeling\n            policy_values = self.policy_head(x).squeeze(-1)  # Output policy values (scalar for each token)\n            \n            return logits, policy_values\n\n\n\n    # ============================\n    # 3. Data Preparation\n    # ============================\n\n    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n    tokenizer.add_special_tokens({'pad_token': '[PAD]', 'bos_token': '[BOS]', 'eos_token': '[EOS]'})\n    dataset = load_dataset(\"togethercomputer/RedPajama-Data-V2\", \"default\", languages=[\"en\"], split=\"train\", streaming=True, trust_remote_code=True)\n    import torch\n    import textstat\n    \n    def compute_fk_reward(decoded_texts):\n        \"\"\"\n        Compute Flesch-Kincaid readability scores as rewards for a batch of decoded texts.\n    \n        Args:\n            decoded_texts (list of str): List of generated texts.\n    \n        Returns:\n            torch.Tensor: FK readability scores normalized to be positive rewards using soft clamping.\n        \"\"\"\n        rewards = []\n        for text in decoded_texts:\n            fk_score = textstat.flesch_kincaid_grade(text)\n    \n            # Convert fk_score and raw_reward to a tensor\n            raw_reward = torch.tensor(fk_score/100, dtype=torch.float32).cuda()\n    \n            # Apply a sigmoid-based function to softly clamp between 0 and 10\n            reward = 10 / (1 + torch.exp(-raw_reward / 2))  # Scaling to control the smoothness\n    \n            rewards.append(raw_reward)\n    \n        # Convert the list of rewards to a torch tensor for batch processing\n        rewards = torch.stack(rewards)  # Stack the list into a single tensor\n        return rewards\n\n\n\n    def collate_fn(batch, tokenizer):\n        tokenized_inputs = [item['tokenized_input'] for item in batch]\n        tokenized_targets = [item['tokenized_target'] for item in batch]\n        bos_token_id = tokenizer.bos_token_id\n        eos_token_id = tokenizer.eos_token_id\n        tokenized_inputs = [\n            torch.cat([torch.tensor([bos_token_id]), tokenized_input, torch.tensor([eos_token_id])])\n            for tokenized_input in tokenized_inputs\n        ]\n        tokenized_targets = [\n            torch.cat([torch.tensor([bos_token_id]), tokenized_target, torch.tensor([eos_token_id])])\n            for tokenized_target in tokenized_targets\n        ]\n        padded_inputs = pad_sequence(tokenized_inputs, batch_first=True, padding_value=tokenizer.pad_token_id)\n        padded_targets = pad_sequence(tokenized_targets, batch_first=True, padding_value=tokenizer.pad_token_id)\n        return padded_inputs, padded_targets\n\n\n    class BufferedStreamingDataLoader:\n        def __init__(self, dataset, batch_size, buffer_size, collate_fn, tokenizer, shuffle_prob=0.3, mask_prob=0.5):\n            self.dataset = dataset\n            self.batch_size = batch_size\n            self.buffer_size = buffer_size\n            self.collate_fn = collate_fn\n            self.tokenizer = tokenizer\n            self.shuffle_prob = shuffle_prob\n            self.mask_prob = mask_prob\n\n        def _split_into_sentences(self, tokenized_input):\n            sentence_end_tokens = {\n                self.tokenizer.convert_tokens_to_ids('.'),\n                self.tokenizer.convert_tokens_to_ids('!'),\n                self.tokenizer.convert_tokens_to_ids('?')\n            }\n            sentences = []\n            current_sentence = []\n            for token in tokenized_input:\n                current_sentence.append(token)\n                if token in sentence_end_tokens:\n                    sentences.append(current_sentence)\n                    current_sentence = []\n            if current_sentence:\n                sentences.append(current_sentence)\n            return sentences\n\n        def _shuffle_sentences(self, tokenized_input):\n            sentences = self._split_into_sentences(tokenized_input)\n            random.shuffle(sentences)\n            shuffled_input = [token for sentence in sentences for token in sentence]\n            return shuffled_input\n\n        def _apply_random_mask(self, tokenized_input):\n            length = len(tokenized_input)\n            if length > 1:\n                mask_start = int(length * 0.33)\n                mask_end = int(length * 0.66)\n                random_mask_start = random.randint(mask_start, mask_end)\n                mask_length = random.randint(int(0.33 * length), int(0.66 * length))\n                mask_end_idx = length\n                masked_input = tokenized_input.clone()\n                mask_token_id = self.tokenizer.pad_token_id\n                masked_input[random_mask_start:mask_end_idx] = mask_token_id\n                return masked_input\n            return tokenized_input\n\n        def __iter__(self):\n            buffer = []\n            token_counter = Counter()\n            for item in self.dataset:\n                tokenized_input = self.tokenizer(\n                    item['raw_content'], return_tensors='pt', max_length=4096, padding=False, truncation=True\n                )['input_ids'][0]\n                tokenized_target = self.tokenizer(\n                    item['raw_content'], return_tensors='pt', max_length=4096, padding=False, truncation=True\n                )['input_ids'][0]\n\n                if random.random() < self.shuffle_prob:\n                    shuffled_input = self._shuffle_sentences(tokenized_input.tolist())\n                    tokenized_input = torch.tensor(shuffled_input)\n                else:\n                    tokenized_input = tokenized_input\n                    if random.random() < self.mask_prob:\n                        tokenized_input = self._apply_random_mask(tokenized_input)\n\n                buffer.append({\n                    'text':  item['raw_content'],\n                    'tokenized_input': tokenized_input,\n                    'tokenized_target': tokenized_target\n                })\n                token_counter.update(tokenized_input.tolist())\n\n                if len(buffer) == self.buffer_size:\n                    buffer = sorted(buffer, key=lambda x: len(x['tokenized_input']), reverse=False)\n                    for i in range(0, len(buffer), self.batch_size):\n                        batch = buffer[i:i + self.batch_size]\n                        yield self.collate_fn(batch, self.tokenizer), token_counter\n                    buffer = []\n                    token_counter = Counter()\n\n            if buffer:\n                buffer = sorted(buffer, key=lambda x: len(x['tokenized_input']), reverse=True)\n                for i in range(0, len(buffer), self.batch_size):\n                    batch = buffer[i:i + self.batch_size]\n                    yield self.collate_fn(batch, self.tokenizer), token_counter\n\n\n    # ============================\n    # 4. Model Initialization\n    # ============================\n\n    def initialize_model_parameters(model):\n        \"\"\" Initialize all trainable parameters in the model. \"\"\"\n        for name, param in model.named_parameters():\n            if 'weight' in name:\n                if len(param.shape) > 1:\n                    nn.init.xavier_uniform_(param)\n                else:\n                    nn.init.normal_(param, 0.0, 0.02)\n            elif 'bias' in name:\n                nn.init.zeros_(param)\n\n\n    batch_size = 1\n    buffer_size = 100 * batch_size\n    dataloader = BufferedStreamingDataLoader(\n        dataset, batch_size=batch_size, buffer_size=buffer_size, collate_fn=collate_fn, tokenizer=tokenizer\n    )\n\n    vocab_size = tokenizer.vocab_size + 3\n    embed_dim = 512\n    d_state = 16\n    d_model = 1024\n    dt_rank = math.ceil(d_model / 16)\n    model = HeadModel(vocab_size, embed_dim, d_state, dt_rank, d_model).cuda()\n    initialize_model_parameters(model)\n    model = nn.DataParallel(model)\n\n\n    # ============================\n    # 5. Optimizer and Scheduler\n    # ============================\n\n    import math\n    import torch\n    from torch.optim import Optimizer\n    import math\n    import torch\n    from torch.optim import Optimizer\n    from torch.autograd import Variable\n\n    class AdEMAMix(Optimizer):\n        def __init__(self, params, lr=1e-3, betas=(0.95, 0.979, 0.9995), eps=1e-7,\n                      weight_decay=0.001, alpha=10.0, T_alpha_beta3=300000, max_steps=300000,\n                      beta_opposite=0.99, nystrom_rank=200, nystrom_interval=50):\n            if not 0.0 <= lr:\n                raise ValueError(f\"Invalid learning rate: {lr}\")\n            if not 0.0 <= eps:\n                raise ValueError(f\"Invalid epsilon value: {eps}\")\n            assert len(betas) == 3, f\"Invalid beta parameters: {betas}, expected 3\"\n            assert all(0.0 <= beta < 1.0 for beta in betas), f\"Invalid beta parameters: {betas}\"\n            if not 0.0 <= weight_decay:\n                raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n\n            defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n                            alpha=alpha, T_alpha_beta3=T_alpha_beta3)\n            super(AdEMAMix, self).__init__(params, defaults)\n\n            self.global_step = 0\n            self.beta1, self.beta2, self.beta3 = betas\n            self.alpha = alpha\n            self.T_alpha_beta3 = T_alpha_beta3\n            self.max_steps = max_steps\n            self.beta_opposite = beta_opposite\n\n            # Nyström parameters\n            self.nystrom_rank = nystrom_rank\n            self.nystrom_interval = nystrom_interval\n            self.gradient_samples = []  # Buffer to store gradient samples\n            self.nystrom_matrix = None  # Approximated covariance matrix\n\n            # Precompute beta powers\n            self.beta1_pows = [1.0]\n            self.beta2_pows = [1.0]\n            self.beta3_pows = [1.0]\n            self.beta_opposite_pows = [1.0]\n            for step in range(1, max_steps + 1):\n                self.beta1_pows.append(self.beta1_pows[-1] * self.beta1)\n                self.beta2_pows.append(self.beta2_pows[-1] * self.beta2)\n                self.beta_opposite_pows.append(self.beta_opposite_pows[-1] * self.beta_opposite)\n                if self.T_alpha_beta3 is not None:\n                    alpha_t = min(step * self.alpha / self.T_alpha_beta3, self.alpha)\n                    exponent = (math.log(self.beta1) * math.log(self.beta3)) / (\n                        (1 - step / self.T_alpha_beta3) * math.log(self.beta3) +\n                        (step / self.T_alpha_beta3) * math.log(self.beta1)\n                    )\n                    beta3_t = min(math.exp(exponent), self.beta3)\n                else:\n                    beta3_t = self.beta3\n                self.beta3_pows.append(beta3_t)\n\n        def __setstate__(self, state):\n            super(AdEMAMix, self).__setstate__(state)\n\n        @torch.no_grad()\n        def step(self, closure=None):\n            loss = None\n            if closure is not None:\n                with torch.enable_grad():\n                    loss = closure()\n\n            self.global_step += 1  # Increment global step once per step\n\n            for group in self.param_groups:\n                params_with_grad = []\n                grads = []\n                exp_avgs = []\n                exp_avg_sqs = []\n                exp_avg_slow = []\n                exp_avg_opposite = []\n\n                for p in group['params']:\n                    if p.grad is not None:\n                        if p.grad.is_sparse:\n                            raise RuntimeError('AdEMAMixNyström does not support sparse gradients')\n                        params_with_grad.append(p)\n                        grads.append(p.grad)\n\n                        state = self.state[p]\n                        if len(state) == 0:\n                            state['step'] = 0\n                            state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                            state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                            state['exp_avg_slow'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                            state['exp_avg_opposite'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n\n                        exp_avgs.append(state['exp_avg'])\n                        exp_avg_sqs.append(state['exp_avg_sq'])\n                        exp_avg_slow.append(state['exp_avg_slow'])\n                        exp_avg_opposite.append(state['exp_avg_opposite'])\n                        state['step'] += 1\n\n                beta1, beta2, beta3 = group['betas']\n                alpha = group['alpha']\n                T_alpha_beta3 = group['T_alpha_beta3']\n\n                step = self.global_step\n                if step <= self.max_steps:\n                    bias_correction1 = 1 - self.beta1_pows[step]\n                    bias_correction2 = 1 - self.beta2_pows[step]\n                    bias_correction_opposite = 1 - self.beta_opposite_pows[step]\n                    if T_alpha_beta3 is not None:\n                        beta3_t = self.beta3_pows[step]\n                    else:\n                        beta3_t = beta3\n                else:\n                    bias_correction1 = 1 - self.beta1 ** step\n                    bias_correction2 = 1 - self.beta2 ** step\n                    bias_correction_opposite = 1 - self.beta_opposite ** step\n                    if T_alpha_beta3 is not None:\n                        alpha_t = min(step * alpha / T_alpha_beta3, alpha)\n                        beta3_t = min(math.exp(math.log(beta1) * math.log(beta3) / \n                                      ((1 - step / T_alpha_beta3) * math.log(beta3) + \n                                        (step / T_alpha_beta3) * math.log(beta1))), self.beta3)\n                    else:\n                        beta3_t = beta3\n\n                # Update Nyström samples\n                if step % self.nystrom_interval == 0:\n                    self._update_nystrom(params_with_grad, grads)\n\n                # Compute preconditioning matrix using Nyström approximation\n                if self.nystrom_matrix is not None:\n                    precondition_matrix = self._compute_preconditioning()\n                else:\n                    precondition_matrix = None\n\n                # Update parameters\n                for i, param in enumerate(params_with_grad):\n                    grad = grads[i]\n                    exp_avg = exp_avgs[i]\n                    exp_avg_sq = exp_avg_sqs[i]\n                    exp_avg_slow_i = exp_avg_slow[i]\n                    exp_avg_opposite_i = exp_avg_opposite[i]\n\n                    # Update biased first moment estimate\n                    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n\n                    # Update opposing EMA\n                    exp_avg_opposite_i.mul_(self.beta_opposite).add_(grad.neg(), alpha=1 - self.beta_opposite)\n\n                    # Update biased second moment estimate\n                    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n                    # Update slow EMA\n                    exp_avg_slow_i.mul_(beta3_t).add_(grad, alpha=1 - beta3_t)\n\n                    # Combine primary and opposing EMAs\n                    combined_momentum = exp_avg + alpha * exp_avg_slow_i + exp_avg_opposite_i\n\n                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n                    step_size = group['lr'] / bias_correction1\n\n                    if group['weight_decay'] != 0:\n                        param.add_(param, alpha=-group['weight_decay'] * group['lr'])\n\n                    # Apply preconditioning if available\n                    if precondition_matrix is not None:\n                        combined_momentum = precondition_matrix.matmul(combined_momentum.view(-1, 1)).view_as(combined_momentum)\n\n                    param.addcdiv_(combined_momentum, denom, value=-step_size)\n\n            return loss\n\n        def _update_nystrom(self, params, grads):\n            \"\"\"\n            Update the gradient samples used for Nyström approximation.\n            \"\"\"\n            # Flatten and concatenate all gradients into a single vector\n            flat_grads = torch.cat([g.view(-1) for g in grads])\n            flat_grads = flat_grads.cpu().detach()\n\n            # Append to samples\n            self.gradient_samples.append(flat_grads)\n\n            # Maintain only the latest nystrom_rank samples\n            if len(self.gradient_samples) > self.nystrom_rank:\n                self.gradient_samples.pop(0)\n\n            # Construct the Nyström matrix if enough samples are available\n            if len(self.gradient_samples) == self.nystrom_rank:\n                # Create a matrix where each column is a sampled gradient\n                G = torch.stack(self.gradient_samples, dim=1)  # Shape: (num_params, nystrom_rank)\n                # Compute the covariance matrix\n                C = G @ G.t() / self.nystrom_rank  # Shape: (num_params, num_params)\n                # Perform eigendecomposition\n                eigenvalues, eigenvectors = torch.linalg.eigh(C)\n                # Select the top k eigenvalues and eigenvectors\n                idx = torch.argsort(eigenvalues, descending=True)[:self.nystrom_rank]\n                self.nystrom_matrix = eigenvectors[:, idx] @ torch.diag(1.0 / torch.sqrt(eigenvalues[idx] + 1e-10)) @ eigenvectors[:, idx].t()\n\n        def _compute_preconditioning(self):\n            \"\"\"\n            Compute the preconditioning matrix using the Nyström approximation.\n            \"\"\"\n            return self.nystrom_matrix.to(next(self.param_groups[0]['params']).device)\n\n\n\n\n\n    optimizer = AdEMAMix(model.parameters(), lr=1e-5)\n    scheduler = CosineAnnealingLR(optimizer, T_max=500000, eta_min=1e-6)\n    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n\n    # ============================\n    # 6. Checkpointing Functions\n    # ============================\n\n    checkpoint_dir = \"/checkpoints\"\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    max_checkpoints = 5  # Maximum number of checkpoints to keep\n\n    def save_model_checkpoint(model, epoch, batch_idx):\n        model_dir = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}_batch_{batch_idx}\")\n        os.makedirs(model_dir, exist_ok=True)\n\n        # Save the entire model using safetensors to handle shared tensors\n        model_to_save = model.module if isinstance(model, nn.DataParallel) else model\n        save_model(model_to_save, os.path.join(model_dir, \"model.safetensors\"))\n\n        cleanup_old_checkpoints()\n\n    def save_optimizer_checkpoint(optimizer, epoch, batch_idx):\n        optimizer_state_file = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}_batch_{batch_idx}\", \"optimizer.pt\")\n        torch.save(optimizer.state_dict(), optimizer_state_file)\n\n    def save_scheduler_checkpoint(scheduler, epoch, batch_idx):\n        scheduler_state_file = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}_batch_{batch_idx}\", \"scheduler.pt\")\n        torch.save(scheduler.state_dict(), scheduler_state_file)\n\n    def save_training_state(epoch, batch_idx, loss_history):\n        training_state_file = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}_batch_{batch_idx}\", \"training_state.pt\")\n        training_state = {\n            'epoch': epoch,\n            'batch_idx': batch_idx,\n            'loss_history': loss_history\n        }\n        torch.save(training_state, training_state_file)\n\n    def cleanup_old_checkpoints():\n        checkpoints = sorted(\n            [d for d in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, d))],\n            key=lambda x: os.path.getctime(os.path.join(checkpoint_dir, x)),\n            reverse=True\n        )\n        if len(checkpoints) > max_checkpoints:\n            old_checkpoints = checkpoints[max_checkpoints:]\n            for old_checkpoint in old_checkpoints:\n                old_checkpoint_path = os.path.join(checkpoint_dir, old_checkpoint)\n                shutil.rmtree(old_checkpoint_path)\n    import os\n    import re\n    import torch\n    from torch import nn\n    from safetensors.torch import load_file  # Import the correct function to load state_dict\n    from safetensors.torch import load_file  # Ensure this import is present\n\n    def load_latest_checkpoint(model, optimizer, scheduler, checkpoint_dir=\"/checkpoints\"):\n        \"\"\"\n        Load the second from the last checkpoint from the specified checkpoint directory.\n\n        Parameters:\n        - model (torch.nn.Module): The model to load the state into.\n        - optimizer (torch.optim.Optimizer): The optimizer to load the state into.\n        - scheduler (torch.optim.lr_scheduler._LRScheduler): The scheduler to load the state into.\n        - checkpoint_dir (str): Directory where checkpoints are stored.\n\n        Returns:\n        - epoch (int): The epoch number to resume from.\n        - batch_idx (int): The batch index to resume from.\n        - loss_history (list): History of loss values.\n        - optimizer (torch.optim.Optimizer): Optimizer with loaded state.\n        - scheduler (torch.optim.lr_scheduler._LRScheduler): Scheduler with loaded state.\n        \"\"\"\n\n        # Ensure the checkpoint directory exists\n        if not os.path.isdir(checkpoint_dir):\n            print(f\"Checkpoint directory '{checkpoint_dir}' does not exist. Starting training from scratch.\")\n            return 0, 0, [], optimizer, scheduler\n\n        # List all checkpoint directories\n        checkpoints = sorted(\n            [d for d in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, d))],\n            key=lambda x: os.path.getctime(os.path.join(checkpoint_dir, x)),\n            reverse=True\n        )\n\n        # Check if there are at least two checkpoints\n        if len(checkpoints) < 2:\n            if len(checkpoints) == 1:\n                print(\"Only one checkpoint found. Loading the latest checkpoint.\")\n                latest_checkpoint = checkpoints[0]\n            else:\n                print(\"No checkpoints found. Starting training from scratch.\")\n                return 0, 0, [], optimizer, scheduler\n        else:\n            # Load the second from the last checkpoint\n            latest_checkpoint = checkpoints[1]\n            print(f\"Resuming training from second latest checkpoint: {latest_checkpoint}\")\n\n        # Extract epoch and batch index from the checkpoint name\n        match = re.match(r\"checkpoint_epoch_(\\d+)_batch_(\\d+)\", latest_checkpoint)\n        if match:\n            epoch = int(match.group(1))\n            batch_idx = int(match.group(2))\n        else:\n            print(\"Checkpoint naming convention not recognized. Starting from scratch.\")\n            return 0, 0, [], optimizer, scheduler\n\n        checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n\n        # Load the model state using safetensors\n        model_to_load = model.module if isinstance(model, nn.DataParallel) else model\n        model_state_path = os.path.join(checkpoint_path, \"model.safetensors\")\n        if os.path.exists(model_state_path):\n            load_model(model_to_load, model_state_path)\n            print(f\"Model state loaded from {model_state_path}.\")\n        else:\n            print(f\"Model state file '{model_state_path}' not found. Starting from scratch.\")\n            return 0, 0, [], optimizer, scheduler\n\n        # Load optimizer state\n        optimizer_state_file = os.path.join(checkpoint_path, \"optimizer.pt\")\n        if os.path.exists(optimizer_state_file):\n            optimizer.load_state_dict(torch.load(optimizer_state_file))\n            print(f\"Optimizer state loaded from {optimizer_state_file}.\")\n        else:\n            print(f\"Optimizer state file '{optimizer_state_file}' not found. Continuing without loading optimizer state.\")\n\n        # Load scheduler state\n        scheduler_state_file = os.path.join(checkpoint_path, \"scheduler.pt\")\n        if os.path.exists(scheduler_state_file):\n            scheduler.load_state_dict(torch.load(scheduler_state_file))\n            print(f\"Scheduler state loaded from {scheduler_state_file}.\")\n        else:\n            print(f\"Scheduler state file '{scheduler_state_file}' not found. Continuing without loading scheduler state.\")\n\n        # Load training state (e.g., loss history)\n        training_state_file = os.path.join(checkpoint_path, \"training_state.pt\")\n        if os.path.exists(training_state_file):\n            training_state = torch.load(training_state_file)\n            loss_history = training_state.get('loss_history', [])\n            print(f\"Training state loaded from {training_state_file}.\")\n        else:\n            loss_history = []\n            print(f\"Training state file '{training_state_file}' not found. Continuing without loading training state.\")\n\n        return 0, batch_idx, loss_history, optimizer, scheduler\n\n\n    # ============================\n    # 7. Training Loop\n    # ============================\n\n    def freeze_all_blocks(model):\n        if isinstance(model, torch.nn.DataParallel):\n            block_stacks = model.module.block_stacks\n        else:\n            block_stacks = model.block_stacks\n\n        # Freeze all blocks and corresponding mlps in each block stack\n        for block_stack in block_stacks:\n            for idx, block in enumerate(block_stack.blocks):\n                # Freeze all parameters in the blocks\n                for param in block.parameters():\n                    param.requires_grad = False\n\n\n    def unfreeze_block_pair(model, block_indices):\n        if isinstance(model, torch.nn.DataParallel):\n            block_stacks = model.module.block_stacks\n        else:\n            block_stacks = model.block_stacks\n\n        # Unfreeze only the specified blocks and corresponding mlps in each block stack\n        for block_stack in block_stacks:\n            for idx in block_indices:\n                # Unfreeze the block\n                for param in block_stack.blocks[idx].parameters():\n                    param.requires_grad = True\n\n\n\n    class BlockPairManager:\n        def __init__(self, num_blocks, pair_size=2):\n            self.num_blocks = num_blocks\n            self.pair_size = pair_size\n            self.block_pairs = self.generate_block_pairs()\n            self.current_pair_idx = 0\n\n        def generate_block_pairs(self):\n            block_pairs = []\n            for i in range(0, self.num_blocks, self.pair_size):\n                pair = tuple(range(i, min(i + self.pair_size, self.num_blocks)))\n                block_pairs.append(pair)\n            return block_pairs\n\n        def get_current_pair(self):\n            return self.block_pairs[self.current_pair_idx]\n\n        def rotate_block_pair(self):\n            self.current_pair_idx = (self.current_pair_idx + 1) % len(self.block_pairs)\n\n        def reset(self):\n            self.current_pair_idx = 0\n\n    # Define accumulation steps\n    accumulation_steps = 8  # Adjust based on your memory constraints\n    # Initialize EMA of rewards (can be zero initially or set to the first batch's rewards)\n    ema = 0.0  # Start at zero or initialize with the first reward batch\n    beta = 0.99  # Smoothing factor for EMA\n\n    num_epochs = 3\n    checkpoint_interval = 200\n    rotation_interval = 500\n    total_batches = 0\n    loss_history = []\n    num_blocks = 1\n    block_pair_manager = BlockPairManager(num_blocks=num_blocks, pair_size=1)\n    start_epoch, start_batch_idx, loss_history, optimizer, scheduler = load_latest_checkpoint(model, optimizer, scheduler)\n    max_sampling_p = 0.9\n    # Gumbel-Softmax Sampling\n    def sample_gumbel_softmax(logits, temperature=1.0, hard=False):\n        \"\"\"\n        Performs Gumbel-Softmax sampling on the logits.\n        \n        Args:\n            logits (torch.Tensor): Logits to sample from.\n            temperature (float): Temperature parameter for Gumbel-Softmax.\n            hard (bool): Whether to return hard (one-hot) samples or soft samples.\n            \n        Returns:\n            torch.Tensor: Sampled tokens in the relaxed form, or one-hot encoded if hard=True.\n        \"\"\"\n        # Sample from the Gumbel distribution\n        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-10) + 1e-10)\n        \n        # Add Gumbel noise to the logits and apply softmax with temperature\n        y_soft = F.softmax((logits + gumbel_noise) / temperature, dim=-1)\n        \n        if hard:\n            # Straight-through trick to obtain discrete tokens\n            y_hard = torch.argmax(y_soft, dim=-1)\n            y_hard_one_hot = F.one_hot(y_hard, num_classes=logits.size(-1)).float()\n            y = (y_hard_one_hot - y_soft).detach() + y_soft  # Differentiable discrete tokens\n        else:\n            y = y_soft\n    \n        return y\n    \n    # Inside your training loop\n    # Inside your training loop\n    for epoch in range(start_epoch, num_epochs):\n        model.train()\n        total_loss = 0\n        total_fk_ease = 0\n        num_batches = 0\n        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}\")\n        freeze_all_blocks(model)\n        unfreeze_block_pair(model, block_pair_manager.get_current_pair())\n    \n        accumulation_counter = 0\n        accumulated_loss = 0.0\n        optimizer.zero_grad()\n    \n        for batch_idx, ((input_ids, labels), freq) in enumerate(progress_bar):\n            if epoch == start_epoch and batch_idx < start_batch_idx:\n                continue\n    \n            input_ids = input_ids.cuda()\n            input_ids = input_ids[:, :-1]\n            labels = labels[:, 1:].contiguous().view(-1).cuda()\n            batch_size, seq_length = input_ids.size()\n            p = min(batch_idx / 18000.0 * max_sampling_p, max_sampling_p)\n            dropout_rate = compute_dropout_rate(batch_idx + total_batches * epoch, 30000)\n    \n            # Forward pass: get both the language model logits and policy values\n            logits, policy_values = model(input_ids, dropout_rate=dropout_rate)\n            logits = logits.view(-1, vocab_size)\n            \n            # Compute Cross Entropy Loss (CE) for language modeling\n            criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n            ce_loss = criterion(logits, labels.view(-1))\n    \n            # Compute policy loss (using Gumbel-Softmax and FK reward advantage)\n            probs = F.softmax(logits, dim=-1)\n            sampled_tokens = torch.argmax(probs, dim=-1)  # Convert to discrete tokens\n            \n            # Decode the sampled tokens to text\n            sampled_tokens = sampled_tokens.view(batch_size, seq_length)\n            decoded_texts = [tokenizer.decode(tokens, skip_special_tokens=True) for tokens in sampled_tokens]\n    \n            # Compute FK reward based on the generated text\n            fk_rewards = compute_fk_reward(decoded_texts)\n            fk_ease = fk_rewards.mean().item()\n    \n            # Update EMA of rewards\n            mean_rewards = fk_rewards.mean()\n            ema = beta * ema + (1 - beta) * mean_rewards  # Update the EMA of rewards\n    \n            # Compute the baseline using the updated EMA\n            baseline = ema\n    \n            # Compute the advantage by subtracting the baseline (EMA) from the FK rewards\n            advantage = fk_rewards - baseline\n    \n            # Compute policy gradient loss using policy values\n            policy_log_probs = torch.log_softmax(policy_values, dim=-1)\n            policy_loss = -(policy_log_probs * advantage).mean()  # Negative log-likelihood weighted by advantage\n    \n            # Total loss: weighted sum of CE loss and policy loss\n            alpha = 0.1  # Weighting factor between CE and RL losses\n            total_loss_batch = ce_loss + alpha * policy_loss\n    \n            # Normalize loss for gradient accumulation\n            total_loss_batch = total_loss_batch / accumulation_steps\n            total_loss_batch.backward()\n    \n            accumulated_loss += total_loss_batch.item() * accumulation_steps\n            total_fk_ease += fk_ease\n            num_batches += 1\n    \n            accumulation_counter += 1\n    \n            if accumulation_counter % accumulation_steps == 0:\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n                accumulation_counter = 0\n    \n            loss_history.append(total_loss_batch.item() * accumulation_steps)\n            avg_loss = accumulated_loss / num_batches\n            avg_fk_ease = total_fk_ease / num_batches\n            progress_bar.set_postfix(\n                loss=total_loss_batch.item() * accumulation_steps, \n                avg_loss=avg_loss,\n                fk_ease=fk_ease,\n                avg_fk_ease=avg_fk_ease\n            )\n            total_batches += 1\n    \n            if total_batches % rotation_interval == 0:\n                freeze_all_blocks(model)\n                block_pair_manager.rotate_block_pair()\n                unfreeze_block_pair(model, block_pair_manager.get_current_pair())\n    \n            if (batch_idx + 1) % checkpoint_interval == 0:\n                # Save checkpoints\n                save_model_checkpoint(model, epoch, batch_idx)\n                save_optimizer_checkpoint(optimizer, epoch, batch_idx)\n                save_scheduler_checkpoint(scheduler, epoch, batch_idx)\n                save_training_state(epoch, batch_idx, loss_history)\n    \n                # Generate and print a sample output\n                model.eval()\n                with torch.no_grad():\n                    input_ids = input_ids[:, :50]\n                    sample_outputs, _ = model(input_ids, dropout_rate=dropout_rate)\n                    sample_outputs = torch.argmax(sample_outputs, dim=-1)\n                    decoded_input = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n                    decoded_text = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n                    print(f\"Batch {batch_idx + 1}: Input Text: {decoded_input} Decoded Text: {decoded_text}\")\n                model.train()\n    \n        if accumulation_counter > 0:\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n    \n        save_model_checkpoint(model, epoch, batch_idx)\n        save_optimizer_checkpoint(optimizer, epoch, batch_idx)\n        save_scheduler_checkpoint(scheduler, epoch, batch_idx)\n        save_training_state(epoch, batch_idx, loss_history)\n        print(f\"Epoch {epoch + 1} completed. Avg Loss: {avg_loss:.4f}, EMA Ease: {ema:.4f}\")\n    \n\n# Run the Modal function\nif __name__ == \"__main__\":\n    with modal.enable_output():\n        with app.run():\n            run_training()\n\n'''\n\n# Specify the file name\nfile_name = \"getting_started.py\"\n\n# Write the content to the file\nwith open(file_name, \"w\") as file:\n    file.write(content)\n\nprint(f\"Python file '{file_name}' has been created successfully.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-12T18:19:47.568869Z","iopub.execute_input":"2024-10-12T18:19:47.569282Z","iopub.status.idle":"2024-10-12T18:19:47.621195Z","shell.execute_reply.started":"2024-10-12T18:19:47.569243Z","shell.execute_reply":"2024-10-12T18:19:47.619979Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Python file 'getting_started.py' has been created successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install einops textstat","metadata":{"execution":{"iopub.status.busy":"2024-10-13T16:31:16.338543Z","iopub.execute_input":"2024-10-13T16:31:16.338919Z","iopub.status.idle":"2024-10-13T16:31:31.882345Z","shell.execute_reply.started":"2024-10-13T16:31:16.338883Z","shell.execute_reply":"2024-10-13T16:31:31.881077Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting textstat\n  Downloading textstat-0.7.4-py3-none-any.whl.metadata (14 kB)\nCollecting pyphen (from textstat)\n  Downloading pyphen-0.16.0-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from textstat) (70.0.0)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading textstat-0.7.4-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyphen-0.16.0-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyphen, einops, textstat\nSuccessfully installed einops-0.8.0 pyphen-0.16.0 textstat-0.7.4\n","output_type":"stream"}]},{"cell_type":"code","source":"    import torch\n    from numba import cuda, float64\n    import math\n    import numpy as np\n    import textstat\n    import torch\n    import torch.nn.functional as F\n    from torch.distributions import Categorical\n    # Ensure that Numba uses the same device as PyTorch\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    # -------------------------------\n    # CUDA Kernels\n    # -------------------------------\n    @cuda.jit\n    def fused_scan_fwd_kernel(u, delta, A, B, C, D, output, b, l, d_in, n):\n        \"\"\"\n        Forward pass kernel implementing Blelloch's inclusive scan.\n        \"\"\"\n        shared_mem = cuda.shared.array(shape=512, dtype=float64)\n        input_x = shared_mem[:256]\n        scan_x = shared_mem[256:]\n        idx = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n        total = b * l * d_in\n        if idx >= total:\n            return\n        dim = idx % d_in\n        seq = (idx // d_in) % l\n        batch = idx // (l * d_in)\n        u_val = u[batch, seq, dim]\n        delta_val = delta[batch, seq, dim]\n        # Initialize scan_x to zero\n        for j in range(n):\n            if j < n:\n                scan_x[j] = 0.0\n        cuda.syncthreads()\n        # Compute scan_x[j] = exp(delta * A[j]) * scan_x[j] + delta * B[j] * u\n        for j in range(n):\n            if j < n:\n                scan_x[j] = math.exp(delta_val * A[dim, j]) * scan_x[j] + delta_val * B[batch, seq, j] * u_val\n        cuda.syncthreads()\n        # Store original scan_x before scan for inclusive scan\n        for j in range(n):\n            if j < n:\n                input_x[j] = scan_x[j]\n        cuda.syncthreads()\n        # Perform Blelloch's exclusive scan on scan_x\n        step = 1\n        while step < n:\n            index = 2 * step * cuda.threadIdx.x + (2 * step - 1)\n            if index < n:\n                scan_x[index] += scan_x[index - step]\n            cuda.syncthreads()\n            step *= 2\n        if cuda.threadIdx.x == 0 and n > 0:\n            scan_x[n - 1] = 0.0\n        cuda.syncthreads()\n        step = n // 2\n        while step >= 1:\n            index = 2 * step * cuda.threadIdx.x + (2 * step - 1)\n            if index < n:\n                t = scan_x[index - step]\n                scan_x[index - step] = scan_x[index]\n                scan_x[index] += t\n            cuda.syncthreads()\n            step //= 2\n        # Inclusive scan: add input_x[j] to scan_x[j]\n        for j in range(n):\n            if j < n:\n                scan_x[j] += input_x[j]\n        cuda.syncthreads()\n        # Now scan_x contains the inclusive scan result\n        # Compute y = sum(scan_x[j] * C[j]) + u * D[dim]\n        y = 0.0\n        for j in range(n):\n            if j < n:\n                y += scan_x[j] * C[batch, seq, j]\n        y += u_val * D[dim]\n        output[batch, seq, dim] = y\n    @cuda.jit\n    def fused_scan_bwd_kernel(u, delta, A, B, C, D, grad_output, grad_u, grad_delta,\n                              grad_A, grad_B, grad_C, grad_D, b, l, d_in, n):\n        \"\"\"\n        Backward pass kernel implementing Blelloch's inclusive scan and reverse scan for gradient computations.\n        \"\"\"\n        # Allocate shared memory: double the size for input and scan\n        shared_mem = cuda.shared.array(shape=1024, dtype=float64)  # Adjust size if needed\n        input_x = shared_mem[:256]\n        scan_x = shared_mem[256:512]\n        grad_shared_A = shared_mem[512:768]\n        grad_shared_B = shared_mem[768:1024]\n        # Initialize shared gradients to zero\n        for j in range(n):\n            if j < n:\n                grad_shared_A[j] = 0.0\n                grad_shared_B[j] = 0.0\n        cuda.syncthreads()\n        idx = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n        total = b * l * d_in\n        if idx >= total:\n            return\n        dim = idx % d_in\n        seq = (idx // d_in) % l\n        batch = idx // (l * d_in)\n        u_val = u[batch, seq, dim]\n        delta_val = delta[batch, seq, dim]\n        y_grad = grad_output[batch, seq, dim]\n        # Initialize scan_x to zero\n        for j in range(n):\n            if j < n:\n                scan_x[j] = 0.0\n        cuda.syncthreads()\n        # Recompute scan_x as in forward pass\n        for j in range(n):\n            if j < n:\n                scan_x[j] = math.exp(delta_val * A[dim, j]) * scan_x[j] + delta_val * B[batch, seq, j] * u_val\n        cuda.syncthreads()\n        # Store original scan_x before scan for inclusive scan\n        for j in range(n):\n            if j < n:\n                input_x[j] = scan_x[j]\n        cuda.syncthreads()\n        # Perform Blelloch's exclusive scan on scan_x\n        step = 1\n        while step < n:\n            index = 2 * step * cuda.threadIdx.x + (2 * step - 1)\n            if index < n:\n                scan_x[index] += scan_x[index - step]\n            cuda.syncthreads()\n            step *= 2\n        if cuda.threadIdx.x == 0 and n > 0:\n            scan_x[n - 1] = 0.0\n        cuda.syncthreads()\n        step = n // 2\n        while step >= 1:\n            index = 2 * step * cuda.threadIdx.x + (2 * step - 1)\n            if index < n:\n                t = scan_x[index - step]\n                scan_x[index - step] = scan_x[index]\n                scan_x[index] += t\n            cuda.syncthreads()\n            step //= 2\n        # Inclusive scan: add input_x[j] to scan_x[j]\n        for j in range(n):\n            if j < n:\n                scan_x[j] += input_x[j]\n        cuda.syncthreads()\n        # Reverse scan to propagate gradients\n        # Initialize gradient accumulation for A and B\n        for j in range(n):\n            if j < n:\n                grad_shared_A[j] = 0.0\n                grad_shared_B[j] = 0.0\n        cuda.syncthreads()\n        # Compute gradients\n        for j in range(n):\n            if j < n:\n                # Gradient w.r.t C[j]\n                grad_C_val = scan_x[j] * y_grad\n                grad_C[batch, seq, j] += grad_C_val\n                # Gradient w.r.t A[j]\n                grad_A_val = delta_val * math.exp(delta_val * A[dim, j]) * scan_x[j] * y_grad\n                grad_shared_A[j] += grad_A_val\n                # Gradient w.r.t B[j]\n                grad_B_val = delta_val * u_val * y_grad\n                grad_shared_B[j] += grad_B_val\n                # Gradient w.r.t D[dim]\n                grad_D[dim] += u_val * y_grad\n        # Now, accumulate shared gradients for A and B\n        for j in range(n):\n            if j < n:\n                A_grad = grad_shared_A[j]\n                B_grad = grad_shared_B[j]\n                grad_A[dim, j] += A_grad\n                grad_B[batch, seq, j] += B_grad\n        # Gradient w.r.t delta\n        for j in range(n):\n            if j < n:\n                grad_delta_val = y_grad * (A[dim, j] * math.exp(delta_val * A[dim, j]) * scan_x[j] +\n                                            B[batch, seq, j] * u_val)\n                grad_delta[batch, seq, dim] += grad_delta_val\n        # Gradient w.r.t u\n        grad_u_val = 0.0\n        for j in range(n):\n            if j < n:\n                grad_u_val += delta_val * B[batch, seq, j] * C[batch, seq, j] * y_grad\n        grad_u[batch, seq, dim] += grad_u_val\n    # -------------------------------\n    # PyTorch Autograd Function\n    # -------------------------------\n    class FusedScanFunction(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, u, delta, A, B, C, D):\n            \"\"\"\n            Forward pass using the custom CUDA kernel.\n            \"\"\"\n            # Detach inputs to prevent PyTorch from tracking gradients through them\n            u_detached = u.detach().contiguous().double().to(device)\n            delta_detached = delta.detach().contiguous().double().to(device)\n            A_detached = A.detach().contiguous().double().to(device)\n            B_detached = B.detach().contiguous().double().to(device)\n            C_detached = C.detach().contiguous().double().to(device)\n            D_detached = D.detach().contiguous().double().to(device)\n            b, l, d_in = u_detached.shape\n            n = A_detached.shape[1]\n            # Allocate output tensor\n            output = torch.zeros_like(u_detached, device=device, dtype=torch.float64)\n            # Define grid and block dimensions\n            threads_per_block = 256\n            blocks_per_grid = (b * l * d_in + (threads_per_block - 1)) // threads_per_block\n            # Launch the forward kernel\n            fused_scan_fwd_kernel[blocks_per_grid, threads_per_block](\n                cuda.as_cuda_array(u_detached),\n                cuda.as_cuda_array(delta_detached),\n                cuda.as_cuda_array(A_detached),\n                cuda.as_cuda_array(B_detached),\n                cuda.as_cuda_array(C_detached),\n                cuda.as_cuda_array(D_detached),\n                cuda.as_cuda_array(output),\n                b, l, d_in, n\n            )\n            # Save context for backward\n            ctx.save_for_backward(u_detached, delta_detached, A_detached, B_detached, C_detached, D_detached)\n            ctx.n = n\n            ctx.b = b\n            ctx.l = l\n            ctx.d_in = d_in\n            return output\n        @staticmethod\n        def backward(ctx, grad_output):\n            u_detached, delta_detached, A_detached, B_detached, C_detached, D_detached = ctx.saved_tensors\n            n = ctx.n\n            b = ctx.b\n            l = ctx.l\n            d_in = ctx.d_in\n            grad_output_detached = grad_output.contiguous().double().to(device)\n            # Allocate gradient tensors\n            grad_u = torch.zeros_like(u_detached, device=device, dtype=torch.float64)\n            grad_delta = torch.zeros_like(delta_detached, device=device, dtype=torch.float64)\n            grad_A = torch.zeros_like(A_detached, device=device, dtype=torch.float64)\n            grad_B = torch.zeros_like(B_detached, device=device, dtype=torch.float64)\n            grad_C = torch.zeros_like(C_detached, device=device, dtype=torch.float64)\n            grad_D = torch.zeros_like(D_detached, device=device, dtype=torch.float64)\n            # Define grid and block dimensions\n            threads_per_block = 256\n            blocks_per_grid = (b * l * d_in + (threads_per_block - 1)) // threads_per_block\n            # Launch the backward kernel\n            fused_scan_bwd_kernel[blocks_per_grid, threads_per_block](\n                cuda.as_cuda_array(u_detached),\n                cuda.as_cuda_array(delta_detached),\n                cuda.as_cuda_array(A_detached),\n                cuda.as_cuda_array(B_detached),\n                cuda.as_cuda_array(C_detached),\n                cuda.as_cuda_array(D_detached),\n                cuda.as_cuda_array(grad_output_detached),\n                cuda.as_cuda_array(grad_u),\n                cuda.as_cuda_array(grad_delta),\n                cuda.as_cuda_array(grad_A),\n                cuda.as_cuda_array(grad_B),\n                cuda.as_cuda_array(grad_C),\n                cuda.as_cuda_array(grad_D),\n                b, l, d_in, n\n            )\n            return grad_u, grad_delta, grad_A, grad_B, grad_C, grad_D\n    # -------------------------------\n    # Custom PyTorch Module\n    # -------------------------------\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    import torch.nn.functional as F\n    from torch.autograd import Function\n    from torch.utils.checkpoint import checkpoint\n    import numpy as np\n    from numba import cuda\n    import math\n    from einops import repeat\n    import sys\n    from torch.optim.lr_scheduler import CosineAnnealingLR\n    import os\n    import math\n    import random\n    import shutil\n    import re\n    from collections import Counter\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.cpp_extension import load\n    from torch.utils.checkpoint import checkpoint\n    from safetensors.torch import save_model, load_model\n    from torch.nn.utils.rnn import pad_sequence\n    from torch.optim import Optimizer\n    from torch.optim.lr_scheduler import CosineAnnealingLR\n    from torch.autograd import Function\n    from einops import repeat\n    from transformers import GPT2Tokenizer\n    from datasets import load_dataset\n    from tqdm import tqdm\n    # ============================\n    # CUDA Kernels\n    # ============================\n    # Wrapper function to launch the forward kernel\n    def fused_scan_fwd(u, delta, A, B, C, D):\n        b, l, d_in = u.shape\n        n = A.shape[1]\n        output = cuda.device_array_like(u)\n        threads_per_block = 256\n        blocks_per_grid = (b * l * d_in + threads_per_block - 1) // threads_per_block\n        blocks_per_grid = max(blocks_per_grid, 2)  # You can adjust this based on performance\n        # shared_mem_size is not needed since we're using fixed shared memory\n        fused_scan_fwd_kernel[blocks_per_grid, threads_per_block](\n            u, delta, A, B, C, D, output, b, l, d_in, n\n        )\n        return output.copy_to_host()\n    # Wrapper function to launch the backward kernel\n    def fused_scan_bwd(u, delta, A, B, C, D, grad_output):\n        b, l, d_in = u.shape\n        n = A.shape[1]\n        grad_u = cuda.device_array_like(u)\n        grad_delta = cuda.device_array_like(delta)\n        grad_A = cuda.device_array_like(A)\n        grad_B = cuda.device_array_like(B)\n        grad_C = cuda.device_array_like(C)\n        grad_D = cuda.device_array_like(D)\n        threads_per_block = 256\n        blocks_per_grid = (b * l * d_in + threads_per_block - 1) // threads_per_block\n        blocks_per_grid = max(blocks_per_grid, 2)  # You can adjust this based on performance\n        # shared_mem_size is not needed since we're using fixed shared memory\n        fused_scan_bwd_kernel[blocks_per_grid, threads_per_block](\n            u, delta, A, B, C, D, grad_output, grad_u, grad_delta, grad_A, grad_B, grad_C, grad_D,\n            b, l, d_in, n\n        )\n        return grad_u.copy_to_host(), grad_delta.copy_to_host(), grad_A.copy_to_host(), grad_B.copy_to_host(), grad_C.copy_to_host(), grad_D.copy_to_host()\n    # ============================\n    # Custom Layers and Functions\n    # ============================\n    def compute_dropout_rate(batch_idx, total_batches, max_dropout=0.4):\n        \"\"\"Compute dropout rate linearly increasing from 0 to max_dropout.\"\"\"\n        return min(max_dropout * (batch_idx / total_batches), max_dropout)\n    class RMSNorm(nn.Module):\n        def __init__(self, d_model: int, eps: float = 1e-5, use_mup: bool = False):\n            super().__init__()\n            self.use_mup = use_mup\n            self.eps = eps\n            # RMSNorm gains prevent muTransfer (section 4.2.3)\n            if not use_mup:\n                self.weight = nn.Parameter(torch.ones(d_model))\n        def forward(self, x):\n            output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n            if not self.use_mup:\n                return output * self.weight\n            else:\n                return output\n    class SelectiveScanFunction(Function):\n        @staticmethod\n        def forward(ctx, u, delta, A_log, B, C, D):\n            A = -torch.exp(A_log.float())\n            # Ensure all tensors are contiguous and on the correct device\n            u_contig = u.contiguous()\n            delta_contig = delta.contiguous()\n            A_contig = A.contiguous()\n            B_contig = B.contiguous()\n            C_contig = C.contiguous()\n            D_contig = D.contiguous()\n            # Move to Numba-compatible device arrays\n            u_device = cuda.to_device(u_contig.cpu().numpy())\n            delta_device = cuda.to_device(delta_contig.cpu().numpy())\n            A_device = cuda.to_device(A_contig.cpu().numpy())\n            B_device = cuda.to_device(B_contig.cpu().numpy())\n            C_device = cuda.to_device(C_contig.cpu().numpy())\n            D_device = cuda.to_device(D_contig.cpu().numpy())\n            # Launch the forward kernel\n            output_host = fused_scan_fwd(u_device, delta_device, A_device, B_device, C_device, D_device)\n            # Convert output back to torch tensor\n            output = torch.from_numpy(output_host).to(u.device)\n            ctx.save_for_backward(u, delta, A_log, B, C, D)\n            return output\n        @staticmethod\n        def backward(ctx, grad_output):\n            u, delta, A_log, B, C, D = ctx.saved_tensors\n            A = -torch.exp(A_log)\n            # Move tensors to CPU and convert to NumPy\n            u_cpu = u.contiguous().cpu().numpy()\n            delta_cpu = delta.contiguous().cpu().numpy()\n            A_cpu = A.contiguous().cpu().numpy()\n            B_cpu = B.contiguous().cpu().numpy()\n            C_cpu = C.contiguous().cpu().numpy()\n            D_cpu = D.contiguous().cpu().numpy()\n            grad_output_cpu = grad_output.contiguous().cpu().numpy()\n            # Move to Numba-compatible device arrays\n            u_device = cuda.to_device(u_cpu)\n            delta_device = cuda.to_device(delta_cpu)\n            A_device = cuda.to_device(A_cpu)\n            B_device = cuda.to_device(B_cpu)\n            C_device = cuda.to_device(C_cpu)\n            D_device = cuda.to_device(D_cpu)\n            grad_output_device = cuda.to_device(grad_output_cpu)\n            # Launch the backward kernel\n            grad_u_host, grad_delta_host, grad_A_host, grad_B_host, grad_C_host, grad_D_host = fused_scan_bwd(\n                u_device, delta_device, A_device, B_device, C_device, D_device, grad_output_device\n            )\n            # Convert gradients back to torch tensors\n            grad_u = torch.from_numpy(grad_u_host).to(u.device)\n            grad_delta = torch.from_numpy(grad_delta_host).to(delta.device)\n            grad_A = torch.from_numpy(grad_A_host).to(A.device)\n            grad_B = torch.from_numpy(grad_B_host).to(B.device)\n            grad_C = torch.from_numpy(grad_C_host).to(C.device)\n            grad_D = torch.from_numpy(grad_D_host).to(D.device)\n            # Compute grad_A_log\n            grad_A_log = grad_A * (-torch.exp(A_log))\n            return grad_u, grad_delta, grad_A_log, grad_B, grad_C, grad_D\n    class SelectiveScanModel(nn.Module):\n        def __init__(self, d_in, d_state, dt_rank):\n            super().__init__()\n            self.norm = RMSNorm(d_in)\n            self.d_state = d_state\n            self.dt_rank = dt_rank\n            A = repeat(\n                torch.arange(1, self.d_state + 1, dtype=torch.float32),\n                \"n -> d n\",\n                d=d_in,\n            ).contiguous()\n            A_log = torch.log(A)\n            self.A_log = nn.Parameter(A_log)\n            self.D = nn.Parameter(torch.ones(d_in).cuda())\n            self.x_proj = nn.Linear(d_in, dt_rank + d_state * 2, bias=True).cuda()\n            self.dt_proj = nn.Linear(dt_rank, d_in, bias=True).cuda()\n            self.output_layer = nn.Linear(d_in, d_in).cuda()\n        def forward(self, x):\n            b, l, d_in = x.shape\n            D = self.D\n            x_dbl = self.x_proj(x)\n            delta, B, C = x_dbl.split([self.dt_rank, self.d_state, self.d_state], dim=-1)\n            delta = F.softplus(self.dt_proj(delta))\n            y = self.norm(SelectiveScanFunction.apply(x, delta, self.A_log, B, C, D))\n            return self.output_layer(y)\n    class MambaBlock(nn.Module):\n        def __init__(self, d_model, d_state=16, dt_rank=16, d_conv=4, expand=2, conv_bias=True, bias=False):\n            super().__init__()\n            self.d_model = d_model\n            self.d_state = d_state\n            self.d_conv = d_conv\n            self.expand = expand\n            self.d_inner = int(self.expand * self.d_model)\n            self.dt_rank = dt_rank\n            self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias)\n            self.conv1d = nn.Sequential(\n                nn.Conv1d(in_channels=self.d_inner, out_channels=self.d_inner, kernel_size=d_conv, groups=self.d_inner, bias=conv_bias),\n                nn.Conv1d(in_channels=self.d_inner, out_channels=self.d_inner, kernel_size=1, bias=conv_bias)\n            )\n            self.activation = nn.SiLU()\n            self.ssm_model = SelectiveScanModel(self.d_inner, self.d_state, self.dt_rank)\n            self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias)\n            self.norm = RMSNorm(self.d_model)\n        def forward(self, x):\n            b, l, d = x.shape\n            x_proj = self.in_proj(x)\n            x, res = x_proj.chunk(2, dim=-1)\n            x = x.transpose(1, 2)\n            x = F.pad(x, (self.d_conv - 1, 0))\n            x = self.conv1d(x)[:, :, :l]\n            x = x.transpose(1, 2)\n            x = self.activation(x)\n            y = self.ssm_model(x)\n            y = y * self.activation(res)\n            y = self.norm(self.out_proj(y))\n            return y\n    class BlockStack(nn.Module):\n        def __init__(self, num_blocks, d_model, d_state, dt_rank):\n            super().__init__()\n            self.blocks = nn.ModuleList([MambaBlock(d_model, d_state, dt_rank) for _ in range(num_blocks)])\n        def forward(self, x, dropout_rate=None):\n            x_res = x\n            for i in range(len(self.blocks)):\n                x = self.blocks[i](x)\n            return x * x_res\n    class FeedForwardLayer(nn.Module):\n        def __init__(self, embed_dim, d_model, hidden_dim, window_size=4):\n            \"\"\"\n            Initializes the FeedForwardLayer with windowed masking.\n            Args:\n                d_model (int): Dimension of the input and output features.\n                hidden_dim (int): Dimension of the hidden layer.\n                window_size (int): Size of the window around each position.\n            \"\"\"\n            super().__init__()\n            self.fc1 = nn.Linear(embed_dim, hidden_dim)\n            self.fc2 = nn.Linear(hidden_dim, d_model)\n            self.activation = nn.SiLU()\n            self.norm = RMSNorm(d_model)\n        def forward(self, x):\n            \"\"\"\n            Forward pass of the FeedForwardLayer with windowed masking.\n            Args:\n                x (torch.Tensor): Input tensor of shape (batch_size, seq_length, d_model).\n            Returns:\n                torch.Tensor: Output tensor of shape (batch_size, seq_length, d_model).\n            \"\"\"\n            # Manually apply the windowed mask to the weights of the first linear layer\n            masked_weight = self.fc1.weight  # Element-wise multiplication\n            masked_bias = self.fc1.bias\n            # Perform the masked linear transformation\n            x = torch.matmul(x, masked_weight.t()) + masked_bias\n            x = self.activation(x)\n            x = self.fc2(x)\n            x = self.norm(x)\n            return x\n    class HeadModel(nn.Module):\n        def __init__(self, vocab_size, embed_dim, d_state, dt_rank, d_model, num_blocks=1, num_matrices=2):\n            super().__init__()\n            self.embedding = nn.Embedding(vocab_size, embed_dim)\n            self.block_stacks = nn.ModuleList([\n                BlockStack(num_blocks, embed_dim // num_matrices, d_state, dt_rank)\n                for _ in range(num_matrices)\n            ])\n            self.activation = nn.SiLU()\n            self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n            self.policy_head = nn.Linear(d_model, 1, bias=False)  # New head for policy output (scalar for each token)\n            self.norm = nn.LayerNorm(d_model)\n            hidden_dim = d_model * 2\n            self.feed_forward = FeedForwardLayer(embed_dim, d_model, hidden_dim)\n            self.num_matrices = num_matrices\n    \n        def forward(self, input_ids, dropout_rate=None):\n            x = self.embedding(input_ids)\n            e = x\n            split_dim = x.shape[2] // self.num_matrices\n            x_splits = torch.split(x, split_dim, dim=2)\n            processed_splits = []\n            for i, feature_matrix in enumerate(x_splits):\n                block_stack = self.block_stacks[i]\n                processed_matrix = checkpoint(block_stack, feature_matrix, use_reentrant=False)\n                processed_splits.append(processed_matrix)\n            x = torch.cat(processed_splits, dim=2)\n            x = self.feed_forward(x + e)\n            x = self.norm(x)\n            \n            logits = self.lm_head(x)  # Output for language modeling\n            policy_values = self.policy_head(x).squeeze(-1)  # Output policy values (scalar for each token)\n            \n            return logits, policy_values\n    # ============================\n    # 3. Data Preparation\n    # ============================\n    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n    tokenizer.add_special_tokens({'pad_token': '[PAD]', 'bos_token': '[BOS]', 'eos_token': '[EOS]'})\n    dataset = load_dataset(\"LLM360/TxT360\", split=\"train\", streaming=True, trust_remote_code=True)\n    import torch\n    import textstat\n    \n    def compute_fk_reward(decoded_texts):\n        \"\"\"\n        Compute Flesch-Kincaid readability scores as rewards for a batch of decoded texts.\n    \n        Args:\n            decoded_texts (list of str): List of generated texts.\n    \n        Returns:\n            torch.Tensor: FK readability scores normalized to be positive rewards using soft clamping.\n        \"\"\"\n        rewards = []\n        for text in decoded_texts:\n            fk_score = textstat.flesch_kincaid_grade(text)\n    \n            # Convert fk_score and raw_reward to a tensor\n            raw_reward = torch.tensor(fk_score/100, dtype=torch.float32).cuda()\n    \n            # Apply a sigmoid-based function to softly clamp between 0 and 10\n            reward = 10 / (1 + torch.exp(-raw_reward / 2))  # Scaling to control the smoothness\n    \n            rewards.append(raw_reward)\n    \n        # Convert the list of rewards to a torch tensor for batch processing\n        rewards = torch.stack(rewards)  # Stack the list into a single tensor\n        return rewards\n    def collate_fn(batch, tokenizer):\n        tokenized_inputs = [item['tokenized_input'] for item in batch]\n        tokenized_targets = [item['tokenized_target'] for item in batch]\n        bos_token_id = tokenizer.bos_token_id\n        eos_token_id = tokenizer.eos_token_id\n        tokenized_inputs = [\n            torch.cat([torch.tensor([bos_token_id]), tokenized_input, torch.tensor([eos_token_id])])\n            for tokenized_input in tokenized_inputs\n        ]\n        tokenized_targets = [\n            torch.cat([torch.tensor([bos_token_id]), tokenized_target, torch.tensor([eos_token_id])])\n            for tokenized_target in tokenized_targets\n        ]\n        padded_inputs = pad_sequence(tokenized_inputs, batch_first=True, padding_value=tokenizer.pad_token_id)\n        padded_targets = pad_sequence(tokenized_targets, batch_first=True, padding_value=tokenizer.pad_token_id)\n        return padded_inputs, padded_targets\n    class BufferedStreamingDataLoader:\n        def __init__(self, dataset, batch_size, buffer_size, collate_fn, tokenizer, shuffle_prob=0.3, mask_prob=0.5):\n            self.dataset = dataset\n            self.batch_size = batch_size\n            self.buffer_size = buffer_size\n            self.collate_fn = collate_fn\n            self.tokenizer = tokenizer\n            self.shuffle_prob = shuffle_prob\n            self.mask_prob = mask_prob\n        def _split_into_sentences(self, tokenized_input):\n            sentence_end_tokens = {\n                self.tokenizer.convert_tokens_to_ids('.'),\n                self.tokenizer.convert_tokens_to_ids('!'),\n                self.tokenizer.convert_tokens_to_ids('?')\n            }\n            sentences = []\n            current_sentence = []\n            for token in tokenized_input:\n                current_sentence.append(token)\n                if token in sentence_end_tokens:\n                    sentences.append(current_sentence)\n                    current_sentence = []\n            if current_sentence:\n                sentences.append(current_sentence)\n            return sentences\n        def _shuffle_sentences(self, tokenized_input):\n            sentences = self._split_into_sentences(tokenized_input)\n            random.shuffle(sentences)\n            shuffled_input = [token for sentence in sentences for token in sentence]\n            return shuffled_input\n        def _apply_random_mask(self, tokenized_input):\n            length = len(tokenized_input)\n            if length > 1:\n                mask_start = int(length * 0.33)\n                mask_end = int(length * 0.66)\n                random_mask_start = random.randint(mask_start, mask_end)\n                mask_length = random.randint(int(0.33 * length), int(0.66 * length))\n                mask_end_idx = length\n                masked_input = tokenized_input.clone()\n                mask_token_id = self.tokenizer.pad_token_id\n                masked_input[random_mask_start:mask_end_idx] = mask_token_id\n                return masked_input\n            return tokenized_input\n        def __iter__(self):\n            buffer = []\n            token_counter = Counter()\n            for item in self.dataset:\n                tokenized_input = self.tokenizer(\n                    item['text'], return_tensors='pt', max_length=4096, padding=False, truncation=True\n                )['input_ids'][0]\n                tokenized_target = self.tokenizer(\n                    item['text'], return_tensors='pt', max_length=4096, padding=False, truncation=True\n                )['input_ids'][0]\n                if random.random() < self.shuffle_prob:\n                    shuffled_input = self._shuffle_sentences(tokenized_input.tolist())\n                    tokenized_input = torch.tensor(shuffled_input)\n                else:\n                    tokenized_input = tokenized_input\n                    if random.random() < self.mask_prob:\n                        tokenized_input = self._apply_random_mask(tokenized_input)\n                buffer.append({\n                    'text':  item['text'],\n                    'tokenized_input': tokenized_input,\n                    'tokenized_target': tokenized_target\n                })\n                token_counter.update(tokenized_input.tolist())\n                if len(buffer) == self.buffer_size:\n                    buffer = sorted(buffer, key=lambda x: len(x['tokenized_input']), reverse=False)\n                    for i in range(0, len(buffer), self.batch_size):\n                        batch = buffer[i:i + self.batch_size]\n                        yield self.collate_fn(batch, self.tokenizer), token_counter\n                    buffer = []\n                    token_counter = Counter()\n            if buffer:\n                buffer = sorted(buffer, key=lambda x: len(x['tokenized_input']), reverse=True)\n                for i in range(0, len(buffer), self.batch_size):\n                    batch = buffer[i:i + self.batch_size]\n                    yield self.collate_fn(batch, self.tokenizer), token_counter\n    # ============================\n    # 4. Model Initialization\n    # ============================\n    def initialize_model_parameters(model):\n        \"\"\" Initialize all trainable parameters in the model. \"\"\"\n        for name, param in model.named_parameters():\n            if 'weight' in name:\n                if len(param.shape) > 1:\n                    nn.init.xavier_uniform_(param)\n                else:\n                    nn.init.normal_(param, 0.0, 0.02)\n            elif 'bias' in name:\n                nn.init.zeros_(param)\n    batch_size = 1\n    buffer_size = 100 * batch_size\n    dataloader = BufferedStreamingDataLoader(\n        dataset, batch_size=batch_size, buffer_size=buffer_size, collate_fn=collate_fn, tokenizer=tokenizer\n    )\n    vocab_size = tokenizer.vocab_size + 3\n    embed_dim = 512\n    d_state = 16\n    d_model = 1024\n    dt_rank = math.ceil(d_model / 16)\n    model = HeadModel(vocab_size, embed_dim, d_state, dt_rank, d_model).cuda()\n    initialize_model_parameters(model)\n    model = nn.DataParallel(model)\n    # ============================\n    # 5. Optimizer and Scheduler\n    # ============================\n    import math\n    import torch\n    from torch.optim import Optimizer\n    import math\n    import torch\n    from torch.optim import Optimizer\n    from torch.autograd import Variable\n    class AdEMAMix(Optimizer):\n        def __init__(self, params, lr=1e-3, betas=(0.95, 0.979, 0.9995), eps=1e-7,\n                      weight_decay=0.001, alpha=10.0, T_alpha_beta3=300000, max_steps=300000,\n                      beta_opposite=0.99, nystrom_rank=200, nystrom_interval=50):\n            if not 0.0 <= lr:\n                raise ValueError(f\"Invalid learning rate: {lr}\")\n            if not 0.0 <= eps:\n                raise ValueError(f\"Invalid epsilon value: {eps}\")\n            assert len(betas) == 3, f\"Invalid beta parameters: {betas}, expected 3\"\n            assert all(0.0 <= beta < 1.0 for beta in betas), f\"Invalid beta parameters: {betas}\"\n            if not 0.0 <= weight_decay:\n                raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n            defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n                            alpha=alpha, T_alpha_beta3=T_alpha_beta3)\n            super(AdEMAMix, self).__init__(params, defaults)\n            self.global_step = 0\n            self.beta1, self.beta2, self.beta3 = betas\n            self.alpha = alpha\n            self.T_alpha_beta3 = T_alpha_beta3\n            self.max_steps = max_steps\n            self.beta_opposite = beta_opposite\n            # Nyström parameters\n            self.nystrom_rank = nystrom_rank\n            self.nystrom_interval = nystrom_interval\n            self.gradient_samples = []  # Buffer to store gradient samples\n            self.nystrom_matrix = None  # Approximated covariance matrix\n            # Precompute beta powers\n            self.beta1_pows = [1.0]\n            self.beta2_pows = [1.0]\n            self.beta3_pows = [1.0]\n            self.beta_opposite_pows = [1.0]\n            for step in range(1, max_steps + 1):\n                self.beta1_pows.append(self.beta1_pows[-1] * self.beta1)\n                self.beta2_pows.append(self.beta2_pows[-1] * self.beta2)\n                self.beta_opposite_pows.append(self.beta_opposite_pows[-1] * self.beta_opposite)\n                if self.T_alpha_beta3 is not None:\n                    alpha_t = min(step * self.alpha / self.T_alpha_beta3, self.alpha)\n                    exponent = (math.log(self.beta1) * math.log(self.beta3)) / (\n                        (1 - step / self.T_alpha_beta3) * math.log(self.beta3) +\n                        (step / self.T_alpha_beta3) * math.log(self.beta1)\n                    )\n                    beta3_t = min(math.exp(exponent), self.beta3)\n                else:\n                    beta3_t = self.beta3\n                self.beta3_pows.append(beta3_t)\n        def __setstate__(self, state):\n            super(AdEMAMix, self).__setstate__(state)\n        @torch.no_grad()\n        def step(self, closure=None):\n            loss = None\n            if closure is not None:\n                with torch.enable_grad():\n                    loss = closure()\n            self.global_step += 1  # Increment global step once per step\n            for group in self.param_groups:\n                params_with_grad = []\n                grads = []\n                exp_avgs = []\n                exp_avg_sqs = []\n                exp_avg_slow = []\n                exp_avg_opposite = []\n                for p in group['params']:\n                    if p.grad is not None:\n                        if p.grad.is_sparse:\n                            raise RuntimeError('AdEMAMixNyström does not support sparse gradients')\n                        params_with_grad.append(p)\n                        grads.append(p.grad)\n                        state = self.state[p]\n                        if len(state) == 0:\n                            state['step'] = 0\n                            state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                            state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                            state['exp_avg_slow'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                            state['exp_avg_opposite'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                        exp_avgs.append(state['exp_avg'])\n                        exp_avg_sqs.append(state['exp_avg_sq'])\n                        exp_avg_slow.append(state['exp_avg_slow'])\n                        exp_avg_opposite.append(state['exp_avg_opposite'])\n                        state['step'] += 1\n                beta1, beta2, beta3 = group['betas']\n                alpha = group['alpha']\n                T_alpha_beta3 = group['T_alpha_beta3']\n                step = self.global_step\n                if step <= self.max_steps:\n                    bias_correction1 = 1 - self.beta1_pows[step]\n                    bias_correction2 = 1 - self.beta2_pows[step]\n                    bias_correction_opposite = 1 - self.beta_opposite_pows[step]\n                    if T_alpha_beta3 is not None:\n                        beta3_t = self.beta3_pows[step]\n                    else:\n                        beta3_t = beta3\n                else:\n                    bias_correction1 = 1 - self.beta1 ** step\n                    bias_correction2 = 1 - self.beta2 ** step\n                    bias_correction_opposite = 1 - self.beta_opposite ** step\n                    if T_alpha_beta3 is not None:\n                        alpha_t = min(step * alpha / T_alpha_beta3, alpha)\n                        beta3_t = min(math.exp(math.log(beta1) * math.log(beta3) / \n                                      ((1 - step / T_alpha_beta3) * math.log(beta3) + \n                                        (step / T_alpha_beta3) * math.log(beta1))), self.beta3)\n                    else:\n                        beta3_t = beta3\n                # Update Nyström samples\n                if step % self.nystrom_interval == 0:\n                    self._update_nystrom(params_with_grad, grads)\n                # Compute preconditioning matrix using Nyström approximation\n                if self.nystrom_matrix is not None:\n                    precondition_matrix = self._compute_preconditioning()\n                else:\n                    precondition_matrix = None\n                # Update parameters\n                for i, param in enumerate(params_with_grad):\n                    grad = grads[i]\n                    exp_avg = exp_avgs[i]\n                    exp_avg_sq = exp_avg_sqs[i]\n                    exp_avg_slow_i = exp_avg_slow[i]\n                    exp_avg_opposite_i = exp_avg_opposite[i]\n                    # Update biased first moment estimate\n                    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                    # Update opposing EMA\n                    exp_avg_opposite_i.mul_(self.beta_opposite).add_(grad.neg(), alpha=1 - self.beta_opposite)\n                    # Update biased second moment estimate\n                    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n                    # Update slow EMA\n                    exp_avg_slow_i.mul_(beta3_t).add_(grad, alpha=1 - beta3_t)\n                    # Combine primary and opposing EMAs\n                    combined_momentum = exp_avg + alpha * exp_avg_slow_i + exp_avg_opposite_i\n                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n                    step_size = group['lr'] / bias_correction1\n                    if group['weight_decay'] != 0:\n                        param.add_(param, alpha=-group['weight_decay'] * group['lr'])\n                    # Apply preconditioning if available\n                    if precondition_matrix is not None:\n                        combined_momentum = precondition_matrix.matmul(combined_momentum.view(-1, 1)).view_as(combined_momentum)\n                    param.addcdiv_(combined_momentum, denom, value=-step_size)\n            return loss\n        def _update_nystrom(self, params, grads):\n            \"\"\"\n            Update the gradient samples used for Nyström approximation.\n            \"\"\"\n            # Flatten and concatenate all gradients into a single vector\n            flat_grads = torch.cat([g.view(-1) for g in grads])\n            flat_grads = flat_grads.cpu().detach()\n            # Append to samples\n            self.gradient_samples.append(flat_grads)\n            # Maintain only the latest nystrom_rank samples\n            if len(self.gradient_samples) > self.nystrom_rank:\n                self.gradient_samples.pop(0)\n            # Construct the Nyström matrix if enough samples are available\n            if len(self.gradient_samples) == self.nystrom_rank:\n                # Create a matrix where each column is a sampled gradient\n                G = torch.stack(self.gradient_samples, dim=1)  # Shape: (num_params, nystrom_rank)\n                # Compute the covariance matrix\n                C = G @ G.t() / self.nystrom_rank  # Shape: (num_params, num_params)\n                # Perform eigendecomposition\n                eigenvalues, eigenvectors = torch.linalg.eigh(C)\n                # Select the top k eigenvalues and eigenvectors\n                idx = torch.argsort(eigenvalues, descending=True)[:self.nystrom_rank]\n                self.nystrom_matrix = eigenvectors[:, idx] @ torch.diag(1.0 / torch.sqrt(eigenvalues[idx] + 1e-10)) @ eigenvectors[:, idx].t()\n        def _compute_preconditioning(self):\n            \"\"\"\n            Compute the preconditioning matrix using the Nyström approximation.\n            \"\"\"\n            return self.nystrom_matrix.to(next(self.param_groups[0]['params']).device)\n    optimizer = AdEMAMix(model.parameters(), lr=1e-5)\n    scheduler = CosineAnnealingLR(optimizer, T_max=500000, eta_min=1e-6)\n    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n    # ============================\n    # 6. Checkpointing Functions\n    # ============================\n    checkpoint_dir=\"/kaggle/working/\"\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    max_checkpoints = 5  # Maximum number of checkpoints to keep\n    def save_model_checkpoint(model, epoch, batch_idx):\n        model_dir = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}_batch_{batch_idx}\")\n        os.makedirs(model_dir, exist_ok=True)\n        # Save the entire model using safetensors to handle shared tensors\n        model_to_save = model.module if isinstance(model, nn.DataParallel) else model\n        save_model(model_to_save, os.path.join(model_dir, \"model.safetensors\"))\n        cleanup_old_checkpoints()\n    def save_optimizer_checkpoint(optimizer, epoch, batch_idx):\n        optimizer_state_file = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}_batch_{batch_idx}\", \"optimizer.pt\")\n        torch.save(optimizer.state_dict(), optimizer_state_file)\n    def save_scheduler_checkpoint(scheduler, epoch, batch_idx):\n        scheduler_state_file = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}_batch_{batch_idx}\", \"scheduler.pt\")\n        torch.save(scheduler.state_dict(), scheduler_state_file)\n    def save_training_state(epoch, batch_idx, loss_history):\n        training_state_file = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}_batch_{batch_idx}\", \"training_state.pt\")\n        training_state = {\n            'epoch': epoch,\n            'batch_idx': batch_idx,\n            'loss_history': loss_history\n        }\n        torch.save(training_state, training_state_file)\n    def cleanup_old_checkpoints():\n        checkpoints = sorted(\n            [d for d in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, d))],\n            key=lambda x: os.path.getctime(os.path.join(checkpoint_dir, x)),\n            reverse=True\n        )\n        if len(checkpoints) > max_checkpoints:\n            old_checkpoints = checkpoints[max_checkpoints:]\n            for old_checkpoint in old_checkpoints:\n                old_checkpoint_path = os.path.join(checkpoint_dir, old_checkpoint)\n                shutil.rmtree(old_checkpoint_path)\n    import os\n    import re\n    import torch\n    from torch import nn\n    from safetensors.torch import load_file  # Import the correct function to load state_dict\n    from safetensors.torch import load_file  # Ensure this import is present\n    def load_latest_checkpoint(model, optimizer, scheduler, checkpoint_dir=\"/kaggle/working/\"):\n        \"\"\"\n        Load the second from the last checkpoint from the specified checkpoint directory.\n        Parameters:\n        - model (torch.nn.Module): The model to load the state into.\n        - optimizer (torch.optim.Optimizer): The optimizer to load the state into.\n        - scheduler (torch.optim.lr_scheduler._LRScheduler): The scheduler to load the state into.\n        - checkpoint_dir (str): Directory where checkpoints are stored.\n        Returns:\n        - epoch (int): The epoch number to resume from.\n        - batch_idx (int): The batch index to resume from.\n        - loss_history (list): History of loss values.\n        - optimizer (torch.optim.Optimizer): Optimizer with loaded state.\n        - scheduler (torch.optim.lr_scheduler._LRScheduler): Scheduler with loaded state.\n        \"\"\"\n        # Ensure the checkpoint directory exists\n        if not os.path.isdir(checkpoint_dir):\n            print(f\"Checkpoint directory '{checkpoint_dir}' does not exist. Starting training from scratch.\")\n            return 0, 0, [], optimizer, scheduler\n        # List all checkpoint directories\n        checkpoints = sorted(\n            [d for d in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, d))],\n            key=lambda x: os.path.getctime(os.path.join(checkpoint_dir, x)),\n            reverse=True\n        )\n        # Check if there are at least two checkpoints\n        if len(checkpoints) < 2:\n            if len(checkpoints) == 1:\n                print(\"Only one checkpoint found. Loading the latest checkpoint.\")\n                latest_checkpoint = checkpoints[0]\n            else:\n                print(\"No checkpoints found. Starting training from scratch.\")\n                return 0, 0, [], optimizer, scheduler\n        else:\n            # Load the second from the last checkpoint\n            latest_checkpoint = checkpoints[1]\n            print(f\"Resuming training from second latest checkpoint: {latest_checkpoint}\")\n        # Extract epoch and batch index from the checkpoint name\n        match = re.match(r\"checkpoint_epoch_(\\d+)_batch_(\\d+)\", latest_checkpoint)\n        if match:\n            epoch = int(match.group(1))\n            batch_idx = int(match.group(2))\n        else:\n            print(\"Checkpoint naming convention not recognized. Starting from scratch.\")\n            return 0, 0, [], optimizer, scheduler\n        checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n        # Load the model state using safetensors\n        model_to_load = model.module if isinstance(model, nn.DataParallel) else model\n        model_state_path = os.path.join(checkpoint_path, \"model.safetensors\")\n        if os.path.exists(model_state_path):\n            load_model(model_to_load, model_state_path)\n            print(f\"Model state loaded from {model_state_path}.\")\n        else:\n            print(f\"Model state file '{model_state_path}' not found. Starting from scratch.\")\n            return 0, 0, [], optimizer, scheduler\n        # Load optimizer state\n        optimizer_state_file = os.path.join(checkpoint_path, \"optimizer.pt\")\n        if os.path.exists(optimizer_state_file):\n            optimizer.load_state_dict(torch.load(optimizer_state_file))\n            print(f\"Optimizer state loaded from {optimizer_state_file}.\")\n        else:\n            print(f\"Optimizer state file '{optimizer_state_file}' not found. Continuing without loading optimizer state.\")\n        # Load scheduler state\n        scheduler_state_file = os.path.join(checkpoint_path, \"scheduler.pt\")\n        if os.path.exists(scheduler_state_file):\n            scheduler.load_state_dict(torch.load(scheduler_state_file))\n            print(f\"Scheduler state loaded from {scheduler_state_file}.\")\n        else:\n            print(f\"Scheduler state file '{scheduler_state_file}' not found. Continuing without loading scheduler state.\")\n        # Load training state (e.g., loss history)\n        training_state_file = os.path.join(checkpoint_path, \"training_state.pt\")\n        if os.path.exists(training_state_file):\n            training_state = torch.load(training_state_file)\n            loss_history = training_state.get('loss_history', [])\n            print(f\"Training state loaded from {training_state_file}.\")\n        else:\n            loss_history = []\n            print(f\"Training state file '{training_state_file}' not found. Continuing without loading training state.\")\n        return 0, 0, loss_history, optimizer, scheduler\n    # ============================\n    # 7. Training Loop\n    # ============================\n    def freeze_all_blocks(model):\n        if isinstance(model, torch.nn.DataParallel):\n            block_stacks = model.module.block_stacks\n        else:\n            block_stacks = model.block_stacks\n        # Freeze all blocks and corresponding mlps in each block stack\n        for block_stack in block_stacks:\n            for idx, block in enumerate(block_stack.blocks):\n                # Freeze all parameters in the blocks\n                for param in block.parameters():\n                    param.requires_grad = False\n    def unfreeze_block_pair(model, block_indices):\n        if isinstance(model, torch.nn.DataParallel):\n            block_stacks = model.module.block_stacks\n        else:\n            block_stacks = model.block_stacks\n        # Unfreeze only the specified blocks and corresponding mlps in each block stack\n        for block_stack in block_stacks:\n            for idx in block_indices:\n                # Unfreeze the block\n                for param in block_stack.blocks[idx].parameters():\n                    param.requires_grad = True\n    class BlockPairManager:\n        def __init__(self, num_blocks, pair_size=2):\n            self.num_blocks = num_blocks\n            self.pair_size = pair_size\n            self.block_pairs = self.generate_block_pairs()\n            self.current_pair_idx = 0\n        def generate_block_pairs(self):\n            block_pairs = []\n            for i in range(0, self.num_blocks, self.pair_size):\n                pair = tuple(range(i, min(i + self.pair_size, self.num_blocks)))\n                block_pairs.append(pair)\n            return block_pairs\n        def get_current_pair(self):\n            return self.block_pairs[self.current_pair_idx]\n        def rotate_block_pair(self):\n            self.current_pair_idx = (self.current_pair_idx + 1) % len(self.block_pairs)\n        def reset(self):\n            self.current_pair_idx = 0\n    # Define accumulation steps\n    accumulation_steps = 8  # Adjust based on your memory constraints\n    # Initialize EMA of rewards (can be zero initially or set to the first batch's rewards)\n    ema = 0.0  # Start at zero or initialize with the first reward batch\n    beta = 0.99  # Smoothing factor for EMA\n    num_epochs = 3\n    checkpoint_interval = 200\n    rotation_interval = 500\n    total_batches = 0\n    loss_history = []\n    num_blocks = 1\n    block_pair_manager = BlockPairManager(num_blocks=num_blocks, pair_size=1)\n    start_epoch, start_batch_idx, loss_history, optimizer, scheduler = load_latest_checkpoint(model, optimizer, scheduler)\n    max_sampling_p = 0.9\n    # Gumbel-Softmax Sampling\n    def sample_gumbel_softmax(logits, temperature=1.0, hard=False):\n        \"\"\"\n        Performs Gumbel-Softmax sampling on the logits.\n        \n        Args:\n            logits (torch.Tensor): Logits to sample from.\n            temperature (float): Temperature parameter for Gumbel-Softmax.\n            hard (bool): Whether to return hard (one-hot) samples or soft samples.\n            \n        Returns:\n            torch.Tensor: Sampled tokens in the relaxed form, or one-hot encoded if hard=True.\n        \"\"\"\n        # Sample from the Gumbel distribution\n        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-10) + 1e-10)\n        \n        # Add Gumbel noise to the logits and apply softmax with temperature\n        y_soft = F.softmax((logits + gumbel_noise) / temperature, dim=-1)\n        \n        if hard:\n            # Straight-through trick to obtain discrete tokens\n            y_hard = torch.argmax(y_soft, dim=-1)\n            y_hard_one_hot = F.one_hot(y_hard, num_classes=logits.size(-1)).float()\n            y = (y_hard_one_hot - y_soft).detach() + y_soft  # Differentiable discrete tokens\n        else:\n            y = y_soft\n    \n        return y\n    \n    # Inside your training loop\n    # Inside your training loop\n    for epoch in range(start_epoch, num_epochs):\n        model.train()\n        total_loss = 0\n        total_fk_ease = 0\n        num_batches = 0\n        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}\")\n        freeze_all_blocks(model)\n        unfreeze_block_pair(model, block_pair_manager.get_current_pair())\n    \n        accumulation_counter = 0\n        accumulated_loss = 0.0\n        optimizer.zero_grad()\n    \n        for batch_idx, ((input_ids, labels), freq) in enumerate(progress_bar):\n            if epoch == start_epoch and batch_idx < start_batch_idx:\n                continue\n    \n            input_ids = input_ids.cuda()\n            input_ids = input_ids[:, :-1]\n            labels = labels[:, 1:].contiguous().view(-1).cuda()\n            batch_size, seq_length = input_ids.size()\n            p = min(batch_idx / 18000.0 * max_sampling_p, max_sampling_p)\n            dropout_rate = compute_dropout_rate(batch_idx + total_batches * epoch, 30000)\n    \n            # Forward pass: get both the language model logits and policy values\n            logits, policy_values = model(input_ids, dropout_rate=dropout_rate)\n            logits = logits.view(-1, vocab_size)\n            \n            # Compute Cross Entropy Loss (CE) for language modeling\n            criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n            ce_loss = criterion(logits, labels.view(-1))\n    \n            # Compute policy loss (using Gumbel-Softmax and FK reward advantage)\n            probs = F.softmax(logits, dim=-1)\n            sampled_tokens = torch.argmax(probs, dim=-1)  # Convert to discrete tokens\n            \n            # Decode the sampled tokens to text\n            sampled_tokens = sampled_tokens.view(batch_size, seq_length)\n            decoded_texts = [tokenizer.decode(tokens, skip_special_tokens=True) for tokens in sampled_tokens]\n    \n            # Compute FK reward based on the generated text\n            fk_rewards = compute_fk_reward(decoded_texts)\n            fk_ease = fk_rewards.mean().item()\n    \n            # Update EMA of rewards\n            mean_rewards = fk_rewards.mean()\n            ema = beta * ema + (1 - beta) * mean_rewards  # Update the EMA of rewards\n    \n            # Compute the baseline using the updated EMA\n            baseline = ema\n    \n            # Compute the advantage by subtracting the baseline (EMA) from the FK rewards\n            advantage = fk_rewards - baseline\n    \n            # Compute policy gradient loss using policy values\n            policy_log_probs = torch.log_softmax(policy_values, dim=-1)\n            policy_loss = -(policy_log_probs * advantage).mean()  # Negative log-likelihood weighted by advantage\n    \n            # Total loss: weighted sum of CE loss and policy loss\n            alpha = 0.01  # Weighting factor between CE and RL losses\n            total_loss_batch = ce_loss + alpha * policy_loss\n    \n            # Normalize loss for gradient accumulation\n            total_loss_batch = total_loss_batch / accumulation_steps\n            total_loss_batch.backward()\n    \n            accumulated_loss += total_loss_batch.item() * accumulation_steps\n            total_fk_ease += fk_ease\n            num_batches += 1\n    \n            accumulation_counter += 1\n    \n            if accumulation_counter % accumulation_steps == 0:\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n                accumulation_counter = 0\n    \n            loss_history.append(total_loss_batch.item() * accumulation_steps)\n            avg_loss = accumulated_loss / num_batches\n            avg_fk_ease = total_fk_ease / num_batches\n            progress_bar.set_postfix(\n                loss=total_loss_batch.item() * accumulation_steps, \n                avg_loss=avg_loss,\n                fk_ease=fk_ease,\n                avg_fk_ease=avg_fk_ease\n            )\n            total_batches += 1\n    \n            if total_batches % rotation_interval == 0:\n                freeze_all_blocks(model)\n                block_pair_manager.rotate_block_pair()\n                unfreeze_block_pair(model, block_pair_manager.get_current_pair())\n    \n            if (batch_idx + 1) % checkpoint_interval == 0:\n                # Save checkpoints\n                save_model_checkpoint(model, epoch, batch_idx)\n                save_optimizer_checkpoint(optimizer, epoch, batch_idx)\n                save_scheduler_checkpoint(scheduler, epoch, batch_idx)\n                save_training_state(epoch, batch_idx, loss_history)\n    \n                # Generate and print a sample output\n                model.eval()\n                with torch.no_grad():\n                    input_ids = input_ids[:, :50]\n                    sample_outputs, _ = model(input_ids, dropout_rate=dropout_rate)\n                    sample_outputs = torch.argmax(sample_outputs, dim=-1)\n                    decoded_input = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n                    decoded_text = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n                    print(f\"Batch {batch_idx + 1}: Input Text: {decoded_input} Decoded Text: {decoded_text}\")\n                model.train()\n    \n        if accumulation_counter > 0:\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n    \n        save_model_checkpoint(model, epoch, batch_idx)\n        save_optimizer_checkpoint(optimizer, epoch, batch_idx)\n        save_scheduler_checkpoint(scheduler, epoch, batch_idx)\n        save_training_state(epoch, batch_idx, loss_history)\n        print(f\"Epoch {epoch + 1} completed. Avg Loss: {avg_loss:.4f}, EMA Ease: {ema:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:55:52.517167Z","iopub.execute_input":"2024-10-13T17:55:52.517553Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/6.74k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce9867907ee14c5ba1dda6664203b733"}},"metadata":{}}]},{"cell_type":"code","source":"!pip install einops textstat ","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:54:10.316576Z","iopub.execute_input":"2024-10-13T17:54:10.317474Z","iopub.status.idle":"2024-10-13T17:54:23.689161Z","shell.execute_reply.started":"2024-10-13T17:54:10.317433Z","shell.execute_reply":"2024-10-13T17:54:23.687989Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting textstat\n  Downloading textstat-0.7.4-py3-none-any.whl.metadata (14 kB)\nCollecting pyphen (from textstat)\n  Downloading pyphen-0.16.0-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from textstat) (70.0.0)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading textstat-0.7.4-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyphen-0.16.0-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyphen, einops, textstat\nSuccessfully installed einops-0.8.0 pyphen-0.16.0 textstat-0.7.4\n","output_type":"stream"}]},{"cell_type":"code","source":"!modal run --detach getting_started.py","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-10-12T18:19:47.623684Z","iopub.execute_input":"2024-10-12T18:19:47.624093Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[2K\u001b[32m✓\u001b[0m Initialized. \u001b[37mView run at \u001b[0m\n\u001b[4;37mhttps://modal.com/apps/efloodproto/main/ap-dk1vDrYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠋\u001b[0m Initializing...\n\u001b[2K\u001b[34m⠸\u001b[0m Creating objects...objects...\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m Creating objects...ggle/working/getting_started.py: Uploaded 0/1 files\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m Creating objects...ing/getting_started.py\n\u001b[37m├── \u001b[0m🔨 Created mount /kaggle/working/getting_started.py\n\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m Creating objects...ning...\n\u001b[37m├── \u001b[0m🔨 Created mount /kaggle/working/getting_started.py\n\u001b[37m└── \u001b[0m🔨 Created function run_training.\n\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m✓\u001b[0m Created objects.\n\u001b[37m├── \u001b[0m🔨 Created mount /kaggle/working/getting_started.py\n\u001b[37m└── \u001b[0m🔨 Created function run_training.\n\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mWorker assigned...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mWorker assigned...\u001b[0m \u001b[37mView app at \u001b[0m\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning...\u001b[0m \u001b[37mView app at \u001b[0mbdT6iy4b\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning...\u001b[0m \u001b[37mView app at \u001b[0mbdT6iy4b\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning...\u001b[0m \u001b[37mView app at \u001b[0mbdT6iy4b\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning...\u001b[0m \u001b[37mView app at \u001b[0mbdT6iy4b\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning...\u001b[0m \u001b[37mView app at \u001b[0mbdT6iy4b\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning...\u001b[0m \u001b[37mView app at \u001b[0mbdT6iy4b\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning...\u001b[0m \u001b[37mView app at \u001b[0mbdT6iy4b\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34mdal.com/apps/efloodproto/main/ap-dk1vDrYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m========== \u001b[0m\u001b[34mLoading images (1 containers initializing)...\u001b[0m\u001b[34m \u001b[0m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/ap\u001b[0m\n== CUDA ==\n\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/ap\u001b[0m\n\u001b[2K\u001b[34m========== \u001b[0m\u001b[34mLoading images (1 containers initializing)...\u001b[0m\u001b[34m \u001b[0m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/ap\u001b[0mm/ap\u001b[0m\n\u001b[2K\u001b[34m⠋\u001b[0m\u001b[34m \u001b[0m\u001b[34mLoading images (1 containers initializing)...\u001b[0m\u001b[34m \u001b[0m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/ap\u001b[0m\n\u001b[2K\u001b[34mCUDA Version 12.4.0mLoading images (1 containers initializing)...\u001b[0m\u001b[34m \u001b[0m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/ap\u001b[0m\n\u001b[2K\u001b[34m⠋\u001b[0m\u001b[34m \u001b[0m\u001b[34mLoading images (1 containers initializing)...\u001b[0m\u001b[34m \u001b[0m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/ap\u001b[0m\nContainer image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/ap\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34mdal.com/apps/efloodproto/main/ap-dk1vDrYkTIO0ExbdT6iy4b\u001b[0m\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\nBy pulling and using the container, you accept the terms and conditions of this license:\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n\nA copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n\u001b[2K\u001b[34m⠹\u001b[0m\u001b[34m \u001b[0m\u001b[34mLoading images (1 containers initializing)...\u001b[0m\u001b[34m \u001b[0m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/ap\u001b[0m\n\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/ap\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31mdal.com/apps/efloodproto/main/ap-dk1vDrYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ADownloading builder script:   0%|          | 0.00/14.1k [00:00<?, ?B/s]\n\u001b[2K\u001b[31m\u001b[1ADownloading builder script: 100%|██████████| 14.1k/14.1k [00:00<00:00, 16.9MB/s] \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31mdal.com/apps/efloodproto/main/ap-dk1vDrYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[31m\u001b[1ADownloading readme:   0%|          | 0.00/42.1k [00:00<?, ?B/s]m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[31m\u001b[1ADownloading readme: 100%|██████████| 42.1k/42.1k [00:00<00:00, 24.8MB/s]w app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[34mResuming training from second latest checkpoint: checkpoint_epoch_0_batch_159399mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[34mModel state loaded from /checkpoints/checkpoint_epoch_0_batch_159399/model.safetensors.pp at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[31m/root/getting_started.py:1188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  optimizer.load_state_dict(torch.load(optimizer_state_file))\n\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m \u001b[0m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34mOptimizer state loaded from /checkpoints/checkpoint_epoch_0_batch_159399/optimizer.pt.\n\u001b[2K\u001b[31m/root/getting_started.py:1196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  scheduler.load_state_dict(torch.load(scheduler_state_file))\n\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[34mScheduler state loaded from /checkpoints/checkpoint_epoch_0_batch_159399/scheduler.pt.app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[31m/root/getting_started.py:1204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  training_state = torch.load(training_state_file)\n\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[34mTraining state loaded from /checkpoints/checkpoint_epoch_0_batch_159399/training_state.pt.at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[31m⠇\u001b[0m\u001b[34m \u001b[0m\u001b[34mRunning (0/1 containers active)...\u001b[0m\u001b[34m \u001b[0m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 0it [00:00, ?it/s]p-dk1vDrYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 1it [00:08,  8.73s/it]1vDrYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 101it [00:09, 13.66it/s]DrYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 201it [00:11, 25.03it/s]DrYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 301it [00:12, 36.84it/s]DrYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 401it [00:14, 41.40it/s]DrYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 501it [00:16, 47.24it/s]DrYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 601it [00:18, 50.05it/s]DrYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 701it [00:19, 57.81it/s]DrYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 801it [00:20, 64.49it/s]DrYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 901it [00:21, 71.68it/s]DrYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 1001it [00:22, 74.95it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 1101it [00:23, 76.76it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 1201it [00:25, 68.67it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 1301it [00:26, 74.08it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 1401it [00:27, 83.97it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 1501it [00:28, 91.16it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 1601it [00:29, 91.80it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 1701it [00:31, 76.19it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m[34mRunning (0/1 containers active)...\u001b[0m\u001b[31m \u001b[0m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31mdal.com/apps/efloodproto/main/ap-dk1vDrYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 1901it [00:34, 72.79it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 2001it [00:35, 83.76it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 2101it [00:35, 90.65it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 2201it [00:37, 80.72it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 2301it [00:38, 77.60it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 2401it [00:40, 71.65it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 2501it [00:42, 67.41it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 2601it [00:43, 68.45it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 2701it [00:44, 73.46it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 2801it [00:46, 74.71it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 2901it [00:46, 84.34it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 3001it [00:48, 77.65it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 3101it [00:49, 77.76it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 3201it [00:50, 78.81it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 3301it [00:51, 85.09it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 3401it [00:53, 76.77it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 3501it [00:55, 69.52it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 3601it [00:57, 62.28it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 3701it [00:58, 67.03it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 3801it [00:59, 73.16it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 3901it [01:01, 70.20it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 4001it [01:02, 71.89it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 4101it [01:04, 61.59it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 4201it [01:05, 67.84it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 4301it [01:06, 74.06it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 4401it [01:08, 73.92it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 4501it [01:08, 83.59it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 4601it [01:10, 76.47it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 4701it [01:11, 80.89it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 4801it [01:12, 79.60it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 4901it [01:14, 80.83it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 5001it [01:15, 76.95it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 5101it [01:16, 79.72it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 5201it [01:18, 70.22it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 5301it [01:19, 70.17it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 5401it [01:21, 75.27it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 5501it [01:21, 84.02it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 5601it [01:23, 82.78it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 5701it [01:24, 87.09it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 5801it [01:25, 83.63it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m[34mRunning (0/1 containers active)...\u001b[0m\u001b[31m \u001b[0m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31mdal.com/apps/efloodproto/main/ap-dk1vDrYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 6001it [01:30, 51.50it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 6101it [01:31, 59.58it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 6201it [01:31, 74.01it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 6301it [01:33, 78.67it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 6401it [01:34, 78.86it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 6501it [01:35, 82.80it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 6601it [01:36, 82.51it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 6701it [01:38, 70.81it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 6801it [01:39, 77.18it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 6901it [01:40, 81.05it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 7001it [01:42, 70.41it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 7101it [01:44, 64.34it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 7201it [01:45, 66.44it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 7301it [01:47, 68.28it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 7401it [01:48, 74.18it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 7501it [01:49, 70.93it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 7601it [01:51, 67.28it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 7701it [01:52, 67.58it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠙\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠼\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 7801it [01:54, 71.03it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠧\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠋\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 7901it [01:55, 76.39it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠸\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠦\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AEpoch 1: 8001it [01:56, 82.10it/s]rYkTIO0ExbdT6iy4b\u001b[0m\n\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/efloodpr\u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠏\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠹\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠴\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[2K\u001b[1A\u001b[2K\u001b[34m⠇\u001b[0m \u001b[34mRunning (0/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n\u001b[4;37mhttps://modal.com/apps/efloodproto/main/ap-dk1vDrYkTIO0ExbdT6iy4b\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}