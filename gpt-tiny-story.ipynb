{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/evanupham/gpt-tiny-story?scriptVersionId=186953109\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom transformers import GPT2Tokenizer, get_linear_schedule_with_warmup\nimport numpy as np\n# Define the custom dataset class\nclass TinyStoriesDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_length):\n        self.texts = [text for text in texts if text.strip() != '']  # Filter out empty sequences\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        tokens = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, truncation=True, padding='max_length')\n        input_ids = tokens.input_ids.squeeze(0)  # Ensure the correct dimension\n        attention_mask = tokens.attention_mask.squeeze(0)  # Ensure the correct dimension\n        return input_ids, attention_mask\n\n# Load the dataset\ndataset = load_dataset('roneneldan/TinyStories')\n\n# Initialize the GPT tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\n\n# Prepare the dataset\nmax_length = 1000\ntrain_texts = dataset['train']['text']\ntrain_dataset = TinyStoriesDataset(train_texts, tokenizer, max_length)\n\n# Create data loader\nbatch_size = 5\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# Define the GPT-2 model with dropout\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:x.size(0), :]\nclass GroupedQueryAttention(nn.Module):\n    def __init__(self, d_model, num_heads, num_groups=2, dropout=0.1):\n        super(GroupedQueryAttention, self).__init__()\n        self.num_heads = num_heads\n        self.num_groups = num_groups\n        self.d_model = d_model\n\n        assert d_model % (num_heads * num_groups) == 0\n\n        self.depth = d_model // (num_heads * num_groups)\n\n        self.wq = nn.Linear(d_model, d_model)\n        self.wk = nn.Linear(d_model, d_model)\n        self.wv = nn.Linear(d_model, d_model)\n\n        self.dense = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def split_heads(self, x, batch_size):\n        x = x.view(batch_size, -1, self.num_groups, self.num_heads, self.depth)\n        return x.permute(0, 2, 3, 1, 4)  # (batch_size, num_groups, num_heads, seq_len, depth)\n\n    def forward(self, q, k, v, mask):\n        batch_size = q.size(0)\n\n        q = self.split_heads(self.wq(q), batch_size)\n        k = self.split_heads(self.wk(k), batch_size)\n        v = self.split_heads(self.wv(v), batch_size)\n\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.depth)\n\n        if mask is not None:\n            mask = mask.unsqueeze(1).unsqueeze(1)  # (batch_size, 1, 1, seq_len, seq_len)\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        attention_weights = torch.nn.functional.softmax(scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n        output = torch.matmul(attention_weights, v)\n\n        output = output.permute(0, 3, 1, 2, 4).contiguous().view(batch_size, -1, self.d_model)\n        output = self.dense(output)\n\n        return output, attention_weights\n\n    \nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.3):\n        super(FeedForward, self).__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.dropout1 = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.dropout1(torch.nn.functional.relu(self.linear1(x)))\n        return self.dropout2(self.linear2(x))\n\nclass GPTBlock(nn.Module):\n    def __init__(self, d_model, num_heads, num_groups, d_ff, dropout=0.3):\n        super(GPTBlock, self).__init__()\n        self.attention = GroupedQueryAttention(d_model, num_heads, num_groups, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.ffn = FeedForward(d_model, d_ff, dropout)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, x, mask):\n        attn_output, _ = self.attention(x, x, x, mask)\n        out1 = self.norm1(x + attn_output)\n        ffn_output = self.ffn(out1)\n        out2 = self.norm2(out1 + ffn_output)\n        return out2\n\nclass GPT2(nn.Module):\n    def __init__(self, vocab_size, d_model, num_heads, num_groups, d_ff, num_layers, max_len=5000, dropout=0.3):\n        super(GPT2, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model, max_len)\n        self.layers = nn.ModuleList([GPTBlock(d_model, num_heads, num_groups, d_ff, dropout) for _ in range(num_layers)])\n        self.norm = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x, mask):\n        x = self.embedding(x)\n        x = self.pos_encoding(x)\n        for layer in self.layers:\n            x = layer(x, mask)\n        x = self.norm(x)\n        x = self.dropout(x)\n        return self.fc(x)\n\ndef create_future_mask(size):\n    mask = torch.tril(torch.ones(size, size)).unsqueeze(0)\n    return mask  # (1, size, size)\n\nvocab_size = len(tokenizer)\nd_model = 768  # GPT-2 small model size\nnum_heads = 6\nd_ff = 3072\nnum_layers = 12\nmax_len = 1024\nnum_groups = 2\nmodel = GPT2(vocab_size, d_model, num_heads, num_groups, d_ff, num_layers, max_len)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nimport os\nmodel_path = \"/kaggle/working/model_weights.pth\"\n# # Load the model weights if they exist\nif os.path.exists(model_path):\n    model.load_state_dict(torch.load(model_path))\n    print(f\"Model weights loaded from {model_path}\")\n\n# Training setup\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\n# Training loop with progress bar\nmodel.train()\n\n# Freeze all layers initially\nfor param in model.parameters():\n    param.requires_grad = False\n\ndef contains_repeated_ngram(seq, n):\n    ngrams = set()\n    for i in range(len(seq) - n + 1):\n        ngram = tuple(seq[i:i+n].tolist())\n        if ngram in ngrams:\n            return True\n        ngrams.add(ngram)\n    return False\n\ndef top_k_top_p_filtering(logits, top_k=0, top_p=1.0, min_p=0.0):\n    \"\"\"Filter a distribution of logits using top-k, top-p (nucleus), and min-p filtering\"\"\"\n    top_k = min(top_k, logits.size(-1))  # Safety check\n    if top_k > 0:\n        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n        logits[indices_to_remove] = -float('Inf')\n\n    if top_p < 1.0:\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n        sorted_indices_to_remove = cumulative_probs > top_p\n        if min_p > 0.0:\n            sorted_indices_to_remove &= (sorted_logits < min_p).cumsum(dim=-1).bool()\n\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n        sorted_indices_to_remove[..., 0] = 0\n\n        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n        logits[indices_to_remove] = -float('Inf')\n        \n    if min_p > 0.0:\n        logits[logits < min_p] = -float('Inf')\n\n    return logits\n\ndef apply_repetition_penalty(logits, seq, repetition_penalty):\n    \"\"\"Apply a penalty to the logits to discourage repetition\"\"\"\n    for token_id in seq:\n        logits[0, token_id] /= repetition_penalty\n    return logits\n\nimport torch\nimport torch.nn.functional as F\nfrom collections import defaultdict\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\ndef calculate_bleu(reference, hypothesis):\n    reference = [reference]  # BLEU expects a list of references\n    smoothie = SmoothingFunction().method4\n    return sentence_bleu(reference, hypothesis, smoothing_function=smoothie)\n\ndef contains_repeated_ngram(seq, n):\n    ngrams = set()\n    for i in range(len(seq) - n + 1):\n        ngram = tuple(seq[i:i+n].tolist())\n        if ngram in ngrams:\n            return True\n        ngrams.add(ngram)\n    return False\n\ndef top_k_top_p_filtering(logits, top_k=0, top_p=1.0, min_p=0.0):\n    \"\"\"Filter a distribution of logits using top-k, top-p (nucleus), and min-p filtering\"\"\"\n    top_k = min(top_k, logits.size(-1))  # Safety check\n    if top_k > 0:\n        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n        logits[indices_to_remove] = -float('Inf')\n\n    if top_p < 1.0:\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n        sorted_indices_to_remove = cumulative_probs > top_p\n        if min_p > 0.0:\n            sorted_indices_to_remove &= (sorted_logits < min_p).cumsum(dim=-1).bool()\n\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n        sorted_indices_to_remove[..., 0] = 0\n\n        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n        logits[indices_to_remove] = -float('Inf')\n        \n    if min_p > 0.0:\n        logits[logits < min_p] = -float('Inf')\n\n    return logits\n\ndef apply_repetition_penalty(logits, seq, repetition_penalty):\n    \"\"\"Apply a penalty to the logits to discourage repetition\"\"\"\n    for token_id in seq:\n        logits[0, token_id] /= repetition_penalty\n    return logits\n\ndef beam_search(model, tokenizer, input_text, beam_width=5, max_len=100, length_penalty=1.2, no_repeat_ngram_size=3, top_k=70, top_p=0.7, min_p=0.1, temperature=0.8, repetition_penalty=1.2, diversity_rate=0.3):\n    input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n    input_ids = input_ids[:, :-1]  # Remove the last token for autoregressive generation\n\n    beam = [(input_ids, 0, [])]  # (input_ids, score, generated tokens)\n    completed_sequences = []\n    diversity_penalty = defaultdict(lambda: 0)\n\n    for step in range(max_len):\n        new_beam = []\n        for seq, score, generated_tokens in beam:\n            with torch.no_grad():\n                outputs = model(seq, create_future_mask(seq.size(1)).to(device))\n            logits = outputs[:, -1, :]  # Get the logits for the last token\n            logits = logits / temperature\n            logits = apply_repetition_penalty(logits, seq[0], repetition_penalty)\n            logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p, min_p=min_p)\n            probs = F.log_softmax(logits, dim=-1)\n            topk_probs, topk_ids = probs.topk(beam_width)\n\n            for i in range(beam_width):\n                next_seq = torch.cat([seq, topk_ids[:, i:i+1]], dim=-1)\n                new_score = score + topk_probs[0, i].item()\n                new_generated_tokens = generated_tokens + [topk_ids[0, i].item()]\n\n                if no_repeat_ngram_size > 0 and contains_repeated_ngram(next_seq[0], no_repeat_ngram_size):\n                    continue  # Skip sequences with repeated n-grams\n\n                # Diversity penalty\n                diversity_penalty[tuple(map(tuple, next_seq.tolist()))] += diversity_rate * step\n                new_score -= diversity_penalty[tuple(map(tuple, next_seq.tolist()))]\n\n                new_beam.append((next_seq, new_score, new_generated_tokens))\n\n        if not new_beam:\n            break  # Break the loop if no new sequences are generated\n\n        beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_width]\n\n        # Check for completed sequences (sequences that have the end token)\n        for seq, score, generated_tokens in beam:\n            if seq[0, -1] == tokenizer.eos_token_id:\n                length_normalized_score = score / (seq.size(1) ** length_penalty)\n                completed_sequences.append((seq, length_normalized_score, generated_tokens))\n\n        # Keep only the sequences that are not completed\n        beam = [b for b in beam if b[0][0, -1] != tokenizer.eos_token_id]\n\n        # Early stopping if all sequences are completed\n        if not beam:\n            break\n\n    if completed_sequences:\n        best_seq = sorted(completed_sequences, key=lambda x: x[1], reverse=True)[0]\n    else:\n        if beam:\n            best_seq = beam[0]  # Fallback to the best beam\n        else:\n            return \"\"  # Return an empty string if no valid sequence is found\n\n    best_seq_tokens = best_seq[2]\n    reference = tokenizer.encode(input_text)  # Use the input text as the reference\n    bleu_score = calculate_bleu(reference, best_seq_tokens)\n\n    output_text = tokenizer.decode(best_seq[0].squeeze(), skip_special_tokens=True)\n    return output_text, bleu_score\n\ndef set_requires_grad(model, layer_idx, requires_grad):\n    for i, layer in enumerate(model.layers):\n        for param in layer.parameters():\n            param.requires_grad = (i == layer_idx) and requires_grad\n\ndef get_custom_training_sequence(num_layers):\n    sequence = []\n    i = 1\n    increment = 1\n    while len(sequence) < 2 * num_layers:\n        sequence.append(i)\n        i += increment\n        if increment == 1:\n            increment = -1\n        else:\n            increment = 2\n        if i > num_layers:\n            i = num_layers - 1\n            increment = 1\n    return sequence\n\nnum_layers = len(model.layers)\ntraining_sequence = get_custom_training_sequence(num_layers)\n\nnum_epochs = len(training_sequence)\ntotal_steps = len(train_loader) * num_epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=total_steps // 10, num_training_steps=total_steps)\ncriterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\ndataset_size = 3000  # Number of samples per epoch\n\ndef parabolic_scaling(epoch, num_epochs):\n    mid_epoch = num_epochs // 2\n    return -2 * ((epoch - mid_epoch) ** 2) / (num_epochs ** 2) + 1\n\n\n# Define the BLEU score calculation function\ndef calculate_bleu(reference, hypothesis):\n    reference = [reference]  # BLEU expects a list of references\n    smoothie = SmoothingFunction().method4\n    return sentence_bleu(reference, hypothesis, smoothing_function=smoothie)\n\ndef z_loss(logits, beta=1e-4):\n    \"\"\"Z-Loss regularizes logits to prevent extreme values.\"\"\"\n    log_z = torch.logsumexp(logits, dim=-1)\n    return beta * log_z.pow(2).mean()\n\nfor epoch in range(num_epochs):\n    # Determine which layer to unfreeze according to the training sequence\n    layer_to_unfreeze = training_sequence[epoch] - 1\n    set_requires_grad(model, layer_to_unfreeze, True)\n    \n    # Create a new subset of the dataset\n    indices = np.random.choice(len(train_dataset), dataset_size, replace=False)\n    subset = Subset(train_dataset, indices)\n    train_loader = DataLoader(subset, batch_size=batch_size, shuffle=True)\n        \n    total_loss = 0\n    total_bleu_score = 0\n    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", postfix={\"Loss\": 0.0000, \"Perplexity\": 0.0000, \"BLEU\": 0.0000})\n    \n    for batch in progress_bar:\n        input_ids, attention_mask = batch\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        \n        # Shift the input for the next token prediction\n        labels = input_ids[:, 1:].contiguous()\n        input_ids = input_ids[:, :-1].contiguous()\n        \n        # Create future mask\n        seq_length = input_ids.size(1)\n        mask = create_future_mask(seq_length).to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids, mask)\n        \n        # Compute loss\n        loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n        \n        # Generate sequences for reward calculation\n        generated_ids = outputs.argmax(dim=-1).cpu().numpy()\n        references = labels.cpu().numpy()\n        batch_bleu_score = 0\n        rewards = []\n        for ref, gen in zip(references, generated_ids):\n            ref_tokens = ref.tolist()\n            gen_tokens = gen.tolist()\n            bleu_score = calculate_bleu(ref_tokens, gen_tokens)\n            rewards.append(bleu_score)\n            batch_bleu_score += bleu_score\n        \n        avg_bleu_score = batch_bleu_score / len(references)\n        \n        # REINFORCE algorithm\n        rewards = torch.tensor(rewards, dtype=torch.float).to(device)\n        log_probs = F.log_softmax(outputs, dim=-1)\n        log_probs = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)\n        policy_loss = -log_probs * rewards.unsqueeze(-1)\n        policy_loss = policy_loss.mean() * parabolic_scaling(epoch, num_epochs)\n        zloss_value = z_loss(outputs)\n        total_loss_with_reward = loss + zloss_value + policy_loss\n        total_loss_with_reward.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        total_loss += total_loss_with_reward.item()\n        total_bleu_score += avg_bleu_score\n        avg_loss = total_loss / len(progress_bar)\n        perplexity = torch.exp(torch.tensor(loss)).item()\n        progress_bar.set_postfix(Loss=f\"{loss.item():.4f}\", Perplexity=f\"{perplexity:.4f}\", BLEU=f\"{avg_bleu_score:.4f}\")\n    \n    avg_loss = total_loss / len(train_loader)\n    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n    avg_bleu_score = total_bleu_score / len(train_loader)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss_with_reward.item():.4f}, Perplexity: {perplexity:.4f}, Avg BLEU: {avg_bleu_score:.4f}\")\n    generated_text, bleu_score = beam_search(model, tokenizer, \"Once upon a time\", beam_width=5, max_len=50)\n    print(f\"Generated text: {generated_text}\")\n    print(f\"BLEU score: {bleu_score:.4f}\")  \n    # Save model weights\n    torch.save(model.state_dict(), model_path)\n    print(f\"Model weights saved to {model_path}\")\n    \n    # Freeze the previously unfrozen layer\n    set_requires_grad(model, layer_to_unfreeze, False)\n\nprint(\"Training complete.\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-05T01:56:07.997494Z","iopub.execute_input":"2024-07-05T01:56:07.997862Z","iopub.status.idle":"2024-07-05T04:15:10.251423Z","shell.execute_reply.started":"2024-07-05T01:56:07.997833Z","shell.execute_reply":"2024-07-05T04:15:10.250202Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"name":"stdout","text":"Model weights loaded from /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/24:   0%|          | 0/600 [00:00<?, ?it/s, BLEU=0, Loss=0, Perplexity=0]/tmp/ipykernel_535/3295353407.py:451: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  perplexity = torch.exp(torch.tensor(loss)).item()\nEpoch 1/24: 100%|██████████| 600/600 [07:41<00:00,  1.30it/s, BLEU=0.7961, Loss=8.5326, Perplexity=5077.5889]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/24, Loss: 9.2613, Perplexity: 10673.1035, Avg BLEU: 0.7908\nGenerated text: Once upon a time, a, there was a little.\n\n\n\". She to the, \". She and the to the. He was a and said, \" it. They and to theSocial, \"\n\nThe to the and. He\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/24: 100%|██████████| 600/600 [07:25<00:00,  1.35it/s, BLEU=0.7515, Loss=8.5365, Perplexity=5097.5640]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/24, Loss: 9.4832, Perplexity: 11884.0752, Avg BLEU: 0.7902\nGenerated text: Once upon a time, there was a little. She to the, \"\n\n\n\". She was a to the and. He. He to the. They, \" and it.\n\nThe and said, \", \"alos and the to the\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/24: 100%|██████████| 600/600 [07:40<00:00,  1.30it/s, BLEU=0.8125, Loss=8.4576, Perplexity=4710.6294]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/24, Loss: 9.3591, Perplexity: 13052.0537, Avg BLEU: 0.7932\nGenerated text: Once upon a time, there was a a little.\n\n\nThe, \". She and to the, \" to the to the. She. He was a and. The to the and said, \"\n\n\", but it was so. They\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/24: 100%|██████████| 600/600 [07:11<00:00,  1.39it/s, BLEU=0.7502, Loss=8.5904, Perplexity=5379.6919]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/24, Loss: 9.7603, Perplexity: 14201.2021, Avg BLEU: 0.7894\nGenerated text: Once upon a time, there was a big. She to the, \"\n\n\nThe. She. He was a to the and the. He, \" and it. They and said, \" identifying and to the.\n\n\" was a unc to\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/24: 100%|██████████| 600/600 [06:41<00:00,  1.49it/s, BLEU=0.8253, Loss=8.5622, Perplexity=5230.0132]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/24, Loss: 9.6049, Perplexity: 15580.8828, Avg BLEU: 0.7908\nGenerated text: Once upon a time, there was a little.\n\n\n\". She and the to the, \", \". He was a She to the and. They. He to the Cumber and1900, \"\n\nThe. The it was a big and\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/24: 100%|██████████| 600/600 [06:12<00:00,  1.61it/s, BLEU=0.8158, Loss=8.6489, Perplexity=5703.7168]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/24, Loss: 9.7896, Perplexity: 16459.5020, Avg BLEU: 0.7911\nGenerated text: Once upon a time, there was a little.\n\n\n\". She to the, \" and the. He was a, \"\n\nThe to the and it. He and said, \". They to the that. She was a big, but\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/24: 100%|██████████| 600/600 [05:43<00:00,  1.74it/s, BLEU=0.8180, Loss=8.3632, Perplexity=4286.2349]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/24, Loss: 9.5320, Perplexity: 17490.5645, Avg BLEU: 0.7930\nGenerated text: Once upon a time, there was a\n\n\nThe, \". She. They and the to the, \" and the. She was a little.\n\n\" to the. He. He was a big and said, \" lot and the, but\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/24: 100%|██████████| 600/600 [05:14<00:00,  1.91it/s, BLEU=0.8352, Loss=8.3906, Perplexity=4405.5327]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/24, Loss: 9.5607, Perplexity: 18269.6738, Avg BLEU: 0.7921\nGenerated text: Once upon a time, there was a, there.\n\n\n\". She was a little to the, \" to the and the. They. She. He and said, \" and and the, but to it was a\n\nThe. He was\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/24: 100%|██████████| 600/600 [05:14<00:00,  1.91it/s, BLEU=0.8219, Loss=8.4899, Perplexity=4865.2246]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/24, Loss: 9.7815, Perplexity: 18930.7188, Avg BLEU: 0.7941\nGenerated text: Once upon a little\nBLEU score: 0.0000\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/24: 100%|██████████| 600/600 [05:00<00:00,  2.00it/s, BLEU=0.6762, Loss=8.3846, Perplexity=4379.2388]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/24, Loss: 9.6660, Perplexity: 19378.0781, Avg BLEU: 0.7937\nGenerated text: Once upon a time, there was a little, \".\n\n\n\". She and the. He was a to the, \" and the, but. She to the Bass. He it was a\nThe. They to the and Aff and said,\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/24: 100%|██████████| 600/600 [05:14<00:00,  1.91it/s, BLEU=0.8155, Loss=8.4125, Perplexity=4503.2202]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/24, Loss: 9.7564, Perplexity: 19851.4121, Avg BLEU: 0.7923\nGenerated text: Once upon a time, there was a little, \". She.\n\n\n\". He to the, \" and the, but and to the it. He was a big. She to the and said, \"\n\nThe. She was a representing\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/24: 100%|██████████| 600/600 [05:14<00:00,  1.91it/s, BLEU=0.7856, Loss=8.4351, Perplexity=4606.1353]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/24, Loss: 9.9045, Perplexity: 19697.5840, Avg BLEU: 0.7949\nGenerated text: Once upon a time, there was a little. She, \".\n\n\nThe and the to the, \" to the. He. He was a big. She was a\n\" and the and it. They, \" and said, but to the\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/24: 100%|██████████| 600/600 [05:00<00:00,  2.00it/s, BLEU=0.8127, Loss=8.4114, Perplexity=4498.0786]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/24, Loss: 9.7854, Perplexity: 19584.6543, Avg BLEU: 0.7924\nGenerated text: Once upon a time, there was a little, \".\n\n\nThe. She was a He to the, \" to the and the. She. He was a\n\n\". They and it was so to the 10000 and said, but. They\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/24: 100%|██████████| 600/600 [05:15<00:00,  1.90it/s, BLEU=0.7447, Loss=8.5353, Perplexity=5091.2432]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/24, Loss: 10.1703, Perplexity: 19122.6035, Avg BLEU: 0.7923\nGenerated text: Once upon a time, there was a little. She to the, \".\n\n\nThe and the. He, \"\n\n\". He was a big. She and the to the park and said, \" it was a to the Yok. They\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/24: 100%|██████████| 600/600 [05:14<00:00,  1.91it/s, BLEU=0.7250, Loss=8.2829, Perplexity=3955.6033]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/24, Loss: 9.9075, Perplexity: 18588.4824, Avg BLEU: 0.7950\nGenerated text: Once upon a time, there was a little, \".\n\n\". She to the, \"\n\n\nThe and the to the. She. He was a big and it and the, but. They to the and said, \" Axel. He\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/24: 100%|██████████| 600/600 [05:00<00:00,  2.00it/s, BLEU=0.8564, Loss=8.3116, Perplexity=4070.6257]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/24, Loss: 9.4119, Perplexity: 18200.5645, Avg BLEU: 0.7917\nGenerated text: Once upon a time, \", there was a little. She was a\n\n\nThe. She to the, \". He and the to the and the\n\" was a big. They. He to the-, \" and said, but. They\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/24: 100%|██████████| 600/600 [05:14<00:00,  1.90it/s, BLEU=0.7349, Loss=8.5682, Perplexity=5261.7314]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/24, Loss: 10.1470, Perplexity: 17006.2598, Avg BLEU: 0.7977\nGenerated text: Once upon a time,\nBLEU score: 0.1337\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/24: 100%|██████████| 600/600 [05:15<00:00,  1.90it/s, BLEU=0.7252, Loss=8.4190, Perplexity=4532.4409]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/24, Loss: 9.7477, Perplexity: 16031.6562, Avg BLEU: 0.7949\nGenerated text: Once upon a time, there was a little, \" to the, \".\n\n\n\". He was a big. She and the She to the\nThe. He. They and said, but and it to the that. She was very to the\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/24: 100%|██████████| 600/600 [05:00<00:00,  1.99it/s, BLEU=0.8272, Loss=8.3917, Perplexity=4410.5059]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/24, Loss: 9.5519, Perplexity: 15072.5605, Avg BLEU: 0.7922\nGenerated text: Once upon a time, there was a little.\n\n\n\" to the, \". He was a big. She to the and said, \" and the park. They. She and the\n\nThe, \"opathy, but it was a window to\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/24: 100%|██████████| 600/600 [05:15<00:00,  1.90it/s, BLEU=0.7770, Loss=8.2794, Perplexity=3941.6328]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/24, Loss: 9.4418, Perplexity: 13881.7734, Avg BLEU: 0.7946\nGenerated text: Once upon a time, there was a little girl, \".\n\n\nThe to the. She was a big. He to the, but and the, \" and it. He was so. She to the\n\n\". They and said, \"\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/24: 100%|██████████| 600/600 [05:15<00:00,  1.90it/s, BLEU=0.8325, Loss=8.4358, Perplexity=4609.0928]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21/24, Loss: 9.4582, Perplexity: 12644.5137, Avg BLEU: 0.7954\nGenerated text: Once upon a time, there was a little, \".\n\n\". He was a big and the, \" to the, but. She and the\n\n\nThe. She was so it to the park and said, \" day, but to the\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/24: 100%|██████████| 600/600 [05:00<00:00,  1.99it/s, BLEU=0.8365, Loss=8.4632, Perplexity=4737.2271]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22/24, Loss: 9.3672, Perplexity: 11464.5439, Avg BLEU: 0.7962\nGenerated text: Once upon a time, there was a little, \". She was a big to the, \"\n\nThe. He and the, but the\n\n\n\". They. She to the Vital and said, but it was very. He was a time\nBLEU score: 0.0873\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/24: 100%|██████████| 600/600 [05:15<00:00,  1.90it/s, BLEU=0.6940, Loss=8.4390, Perplexity=4623.7793]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23/24, Loss: 9.6457, Perplexity: 10315.8906, Avg BLEU: 0.7940\nGenerated text: Once upon a time, there was a little\nBLEU score: 0.2368\nModel weights saved to /kaggle/working/model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/24: 100%|██████████| 600/600 [05:15<00:00,  1.90it/s, BLEU=0.8083, Loss=8.5683, Perplexity=5262.2480]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24/24, Loss: 9.4212, Perplexity: 9237.1982, Avg BLEU: 0.7938\nGenerated text: Once upon a time, there was a little girl.\n\n\n\". She was a big, \" to the, \"\nThe and the park. He was very to the park and said, but. She. They and the\n\nThe to the\nBLEU score: 0.1881\nModel weights saved to /kaggle/working/model_weights.pth\nTraining complete.\n","output_type":"stream"}]},{"cell_type":"code","source":"model_path = \"model_weights.pth\"\ntorch.save(model.state_dict(), model_path)\nprint(f\"Model weights saved to {model_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T04:15:10.253878Z","iopub.execute_input":"2024-07-05T04:15:10.254189Z","iopub.status.idle":"2024-07-05T04:15:11.699769Z","shell.execute_reply.started":"2024-07-05T04:15:10.254163Z","shell.execute_reply":"2024-07-05T04:15:11.698412Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Model weights saved to model_weights.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom collections import defaultdict\n\ndef create_future_mask(size):\n    mask = torch.triu(torch.ones(size, size), diagonal=1).bool()\n    return mask.to(device)\n\nimport torch\nimport torch.nn.functional as F\nfrom collections import defaultdict\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\ndef calculate_bleu(reference, hypothesis):\n    reference = [reference]  # BLEU expects a list of references\n    smoothie = SmoothingFunction().method4\n    return sentence_bleu(reference, hypothesis, smoothing_function=smoothie)\n\ndef contains_repeated_ngram(seq, n):\n    ngrams = set()\n    for i in range(len(seq) - n + 1):\n        ngram = tuple(seq[i:i+n].tolist())\n        if ngram in ngrams:\n            return True\n        ngrams.add(ngram)\n    return False\n\ndef top_k_top_p_filtering(logits, top_k=0, top_p=1.0, min_p=0.0):\n    \"\"\"Filter a distribution of logits using top-k, top-p (nucleus), and min-p filtering\"\"\"\n    top_k = min(top_k, logits.size(-1))  # Safety check\n    if top_k > 0:\n        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n        logits[indices_to_remove] = -float('Inf')\n\n    if top_p < 1.0:\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n        sorted_indices_to_remove = cumulative_probs > top_p\n        if min_p > 0.0:\n            sorted_indices_to_remove &= (sorted_logits < min_p).cumsum(dim=-1).bool()\n\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n        sorted_indices_to_remove[..., 0] = 0\n\n        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n        logits[indices_to_remove] = -float('Inf')\n        \n    if min_p > 0.0:\n        logits[logits < min_p] = -float('Inf')\n\n    return logits\n\ndef apply_repetition_penalty(logits, seq, repetition_penalty):\n    \"\"\"Apply a penalty to the logits to discourage repetition\"\"\"\n    for token_id in seq:\n        logits[0, token_id] /= repetition_penalty\n    return logits\n\ndef beam_search(model, tokenizer, input_text, beam_width=5, max_len=50, length_penalty=1.0, no_repeat_ngram_size=3, top_k=50, top_p=0.8, min_p=0.2, temperature=0.6, repetition_penalty=1.1, diversity_rate=0.1):\n    input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n    input_ids = input_ids[:, :-1]  # Remove the last token for autoregressive generation\n\n    beam = [(input_ids, 0, [])]  # (input_ids, score, generated tokens)\n    completed_sequences = []\n    diversity_penalty = defaultdict(lambda: 0)\n\n    for step in range(max_len):\n        new_beam = []\n        for seq, score, generated_tokens in beam:\n            with torch.no_grad():\n                outputs = model(seq, create_future_mask(seq.size(1)).to(device))\n            logits = outputs[:, -1, :]  # Get the logits for the last token\n            logits = logits / temperature\n            logits = apply_repetition_penalty(logits, seq[0], repetition_penalty)\n            logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p, min_p=min_p)\n            probs = F.log_softmax(logits, dim=-1)\n            topk_probs, topk_ids = probs.topk(beam_width)\n\n            for i in range(beam_width):\n                next_seq = torch.cat([seq, topk_ids[:, i:i+1]], dim=-1)\n                new_score = score + topk_probs[0, i].item()\n                new_generated_tokens = generated_tokens + [topk_ids[0, i].item()]\n\n                if no_repeat_ngram_size > 0 and contains_repeated_ngram(next_seq[0], no_repeat_ngram_size):\n                    continue  # Skip sequences with repeated n-grams\n\n                # Diversity penalty\n                diversity_penalty[tuple(map(tuple, next_seq.tolist()))] += diversity_rate * step\n                new_score -= diversity_penalty[tuple(map(tuple, next_seq.tolist()))]\n\n                new_beam.append((next_seq, new_score, new_generated_tokens))\n\n        if not new_beam:\n            break  # Break the loop if no new sequences are generated\n\n        beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_width]\n\n        # Check for completed sequences (sequences that have the end token)\n        for seq, score, generated_tokens in beam:\n            if seq[0, -1] == tokenizer.eos_token_id:\n                length_normalized_score = score / (seq.size(1) ** length_penalty)\n                completed_sequences.append((seq, length_normalized_score, generated_tokens))\n\n        # Keep only the sequences that are not completed\n        beam = [b for b in beam if b[0][0, -1] != tokenizer.eos_token_id]\n\n        # Early stopping if all sequences are completed\n        if not beam:\n            break\n\n    if completed_sequences:\n        best_seq = sorted(completed_sequences, key=lambda x: x[1], reverse=True)[0]\n    else:\n        if beam:\n            best_seq = beam[0]  # Fallback to the best beam\n        else:\n            return \"\"  # Return an empty string if no valid sequence is found\n\n    best_seq_tokens = best_seq[2]\n    reference = tokenizer.encode(input_text)  # Use the input text as the reference\n    bleu_score = calculate_bleu(reference, best_seq_tokens)\n\n    output_text = tokenizer.decode(best_seq[0].squeeze(), skip_special_tokens=True)\n    return output_text, bleu_score\n\n\n\ngenerated_text, bleu_score = beam_search(model, tokenizer, \"Once upon a time\", beam_width=5, max_len=50)\nprint(f\"Generated text: {generated_text}\")\nprint(f\"BLEU score: {bleu_score:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T04:15:11.701862Z","iopub.execute_input":"2024-07-05T04:15:11.702245Z","iopub.status.idle":"2024-07-05T04:15:12.328952Z","shell.execute_reply.started":"2024-07-05T04:15:11.702211Z","shell.execute_reply":"2024-07-05T04:15:12.327605Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 130\u001b[0m\n\u001b[1;32m    125\u001b[0m     output_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(best_seq[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(), skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_text, bleu_score\n\u001b[0;32m--> 130\u001b[0m generated_text, bleu_score \u001b[38;5;241m=\u001b[39m \u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOnce upon a time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbleu_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[6], line 73\u001b[0m, in \u001b[0;36mbeam_search\u001b[0;34m(model, tokenizer, input_text, beam_width, max_len, length_penalty, no_repeat_ngram_size, top_k, top_p, min_p, temperature, repetition_penalty, diversity_rate)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq, score, generated_tokens \u001b[38;5;129;01min\u001b[39;00m beam:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 73\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_future_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Get the logits for the last token\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m/\u001b[39m temperature\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[4], line 147\u001b[0m, in \u001b[0;36mGPT2.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    145\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding(x)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 147\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    149\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[4], line 127\u001b[0m, in \u001b[0;36mGPTBlock.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[0;32m--> 127\u001b[0m     attn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m attn_output)\n\u001b[1;32m    129\u001b[0m     ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(out1)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[4], line 94\u001b[0m, in \u001b[0;36mGroupedQueryAttention.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, 1, 1, seq_len, seq_len)\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1e9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     97\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_weights)\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"],"ename":"RuntimeError","evalue":"The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1","output_type":"error"}]},{"cell_type":"code","source":"!pip install einops","metadata":{"execution":{"iopub.status.busy":"2024-07-05T04:15:12.329897Z","iopub.status.idle":"2024-07-05T04:15:12.330238Z","shell.execute_reply.started":"2024-07-05T04:15:12.330076Z","shell.execute_reply":"2024-07-05T04:15:12.33009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}