{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eupham/-/blob/master/untitled24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKGzAusMCbjy"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install datasets\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import namedtuple\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.data import get_tokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths to save and load files\n",
        "model_path = '/content/checkpoint_epoch0_batch1000.pth'\n",
        "vocab_path = '/vocab.pkl'\n",
        "word_to_index_path = '/word_to_index.pkl'\n",
        "\n",
        "# 1. Embedding Layer\n",
        "class EmbeddingLayer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super(EmbeddingLayer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x)\n",
        "\n",
        "# 2. Transformer Encoder\n",
        "class TransformerEncoderModule(nn.Module):\n",
        "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
        "        super(TransformerEncoderModule, self).__init__()\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.transformer_encoder(x)\n",
        "\n",
        "# 3. Classification Head\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, d_model, num_classes):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.gru = nn.GRU(d_model, d_model // 2, batch_first=True, bidirectional=True)\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply GRU\n",
        "        x, _ = self.gru(x)\n",
        "\n",
        "        # Apply Leaky ReLU activation\n",
        "        x = self.leaky_relu(x)\n",
        "\n",
        "        # Apply fully connected layer\n",
        "        x = self.fc(x)\n",
        "\n",
        "        # Apply softmax\n",
        "        return self.softmax(x)\n",
        "\n",
        "\n",
        "\n",
        "# 4. Sequence Generation Head\n",
        "class SequenceGenerationHead(nn.Module):\n",
        "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, vocab_size):\n",
        "        super(SequenceGenerationHead, self).__init__()\n",
        "        decoder_layers = TransformerDecoderLayer(d_model, nhead, dim_feedforward)\n",
        "        self.transformer_decoder = TransformerDecoder(decoder_layers, num_layers)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, memory):\n",
        "        output = self.transformer_decoder(x, memory)\n",
        "        return self.fc_out(output)\n",
        "\n",
        "# 5. Complete Model\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward, num_classes):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = EmbeddingLayer(vocab_size, d_model)\n",
        "        self.encoder = TransformerEncoderModule(d_model, nhead, num_layers, dim_feedforward)\n",
        "        self.classification_head = ClassificationHead(d_model, num_classes)\n",
        "        self.sequence_generation_head = SequenceGenerationHead(d_model, nhead, num_layers, dim_feedforward, vocab_size)\n",
        "\n",
        "    def forward(self, x, task='classification'):\n",
        "        embedded = self.embedding(x)\n",
        "        encoded = self.encoder(embedded)\n",
        "        if task == 'classification':\n",
        "            return self.classification_head(encoded)\n",
        "        elif task == 'generation':\n",
        "            return self.sequence_generation_head(embedded, encoded)\n",
        "\n",
        "# 6. Dataset with Fake Target Column\n",
        "squad_dataset = load_dataset(\"squad\")\n",
        "contexts = [item['context'] for item in squad_dataset['train']]\n",
        "questions = [item['question'] for item in squad_dataset['train']]\n",
        "titles = [item['title'] for item in squad_dataset['train']]\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_titles = label_encoder.fit_transform(titles)\n",
        "sentences = list(zip(contexts, encoded_titles, questions))\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "tokenized_sentences = [tokenizer(sentence) for sentence, _, _ in sentences]\n",
        "tokenized_targets = [tokenizer(target) for _, _, target in sentences]\n",
        "vocab = set(token for sentence in tokenized_sentences + tokenized_targets for token in sentence)\n",
        "vocab_size = len(vocab)\n",
        "word_to_index = {word: index for index, word in enumerate(vocab)}\n",
        "indexed_sentences = [[word_to_index[token] for token in sentence] for sentence in tokenized_sentences]\n",
        "indexed_targets = [[word_to_index[token] for token in target] for target in tokenized_targets]\n",
        "Sentence = namedtuple('Sentence', ['text', 'label', 'target'])\n",
        "dataset = [Sentence(torch.tensor([word_to_index[token] for token in sentence]), label, torch.tensor([word_to_index[token] for token in target])) for sentence, label, target in zip(tokenized_sentences, [label for _, label, _ in sentences], tokenized_targets)]\n",
        "\n",
        "# Custom Dataset class\n",
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, sentences):\n",
        "        self.sentences = sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentences[idx]\n",
        "\n",
        "# Collate function for padding\n",
        "def collate_fn(batch):\n",
        "    texts = [item.text for item in batch]\n",
        "    labels = torch.tensor([item.label for item in batch])\n",
        "    targets = [item.target for item in batch]\n",
        "    texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
        "    targets = pad_sequence(targets, batch_first=True, padding_value=0)\n",
        "    return texts, labels, targets\n",
        "\n",
        "# DataLoader\n",
        "dataloader = DataLoader(SentenceDataset(dataset), batch_size=2, collate_fn=collate_fn)\n",
        "\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "# 7. Training Loop\n",
        "d_model = 1536  # Model dimension, should be divisible by nhead\n",
        "nhead = 12     # Number of attention heads in each layer\n",
        "num_layers = 12 # Number of layers\n",
        "dim_feedforward = 3072 # Dimension of the feedforward network model\n",
        "num_classes = len(set(encoded_titles))\n",
        "num_epochs = 1\n",
        "\n",
        "model = TransformerModel(vocab_size, d_model, nhead, num_layers, dim_feedforward, num_classes)\n",
        "model.to(device) # Move the model to the device\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "classification_loss_fn = nn.CrossEntropyLoss()\n",
        "generation_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize a variable to keep track of the last checkpoint time\n",
        "last_checkpoint_time = time.time()\n",
        "\n",
        "# Define the checkpoint interval\n",
        "checkpoint_interval = 200\n",
        "\n",
        "# Wrap the range with tqdm to show progress bar\n",
        "for epoch in tqdm(range(num_epochs), desc='Epochs'):\n",
        "    # Create an inner tqdm loop for batches, with set_postfix enabled\n",
        "    batch_iterator = tqdm(enumerate(dataloader), total=len(dataloader), desc='Batches', leave=False)\n",
        "    for batch_idx, batch in batch_iterator:\n",
        "        input_data, labels, targets = batch\n",
        "        input_data, labels, targets = input_data.to(device), labels.to(device), targets.to(device) # Move to device\n",
        "\n",
        "        # Forward pass for classification\n",
        "        classification_output = model(input_data, task='classification')\n",
        "        classification_loss = classification_loss_fn(classification_output[:, -1, :], labels)\n",
        "\n",
        "        # Optimization for classification\n",
        "        optimizer.zero_grad()\n",
        "        classification_loss.backward(retain_graph=True) # Retain the graph for next step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Forward pass for sequence generation\n",
        "        generation_output = model(input_data, task='generation')\n",
        "\n",
        "        # Reshape the generation_output and targets\n",
        "        generation_output_reshaped = generation_output.view(-1, vocab_size)\n",
        "        targets_aligned = targets[:, :generation_output.size(1)]\n",
        "        targets_reshaped = targets_aligned.view(-1)\n",
        "\n",
        "        # Ensure the generation_output and targets have the same shape\n",
        "        if generation_output_reshaped.size(0) != targets_reshaped.size(0):\n",
        "            generation_output_reshaped = generation_output_reshaped[:targets_reshaped.size(0), :]\n",
        "\n",
        "        generation_loss = generation_loss_fn(generation_output_reshaped, targets_reshaped)\n",
        "\n",
        "        # Optimization for sequence generation\n",
        "        optimizer.zero_grad()\n",
        "        generation_loss.backward() # No need to retain the graph here\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the postfix of the tqdm loop with the current losses\n",
        "        batch_iterator.set_postfix({\n",
        "            'classification_loss': classification_loss.item(),\n",
        "            'generation_loss': generation_loss.item(),\n",
        "        })\n",
        "\n",
        "        # Checkpoint the model every checkpoint_interval batches\n",
        "        if batch_idx % checkpoint_interval == 0:\n",
        "            checkpoint_path = f'checkpoint_epoch{epoch}_batch{batch_idx}.pth'\n",
        "            torch.save(model.state_dict(), checkpoint_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUhdabpkIdlEGuwqaP9ZpW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}